{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cee9746",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aarondomenzain/tracking-softmatter-aarond/blob/tracking-softmatter-aarond-dev/tutorial/detection/detection_spheres.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5389-e5c9-4ff1-8ab7-c8c998a829d2",
   "metadata": {},
   "source": [
    "# Particle tracking tutorial: Detection of particles\n",
    "\n",
    "This tutorial compares different methods for the detection of particles from image files, both from experimental data and simulations. It relies on a number of Python packages, including NumPy, SciKit, Matplotlib, PyTorch, Deeptrack, and Deeplay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da54cc",
   "metadata": {},
   "source": [
    "### 1. Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817b977",
   "metadata": {},
   "source": [
    "Uncomment the next cell if running on Google Colab/Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c86909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deeptrack deeplay trackpy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab7e2d",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3c704-1473-4998-bf85-e07aa6ff1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries.\n",
    "#import math  # Mathematical operations.\n",
    "import os  # Operating system dependent functionality.\n",
    "import random  # Generate random numbers.\n",
    "import sys  # System-specific parameters and functions.\n",
    "\n",
    "# External libraries.\n",
    "#import cv2  # OpenCV (Computer Vision).\n",
    "import deeptrack as dt  # DeepTrack.\n",
    "import deeplay as dl  # Deeplay.\n",
    "from deeptrack.extras.radialcenter import radialcenter as rc \n",
    "from itertools import cycle # Iterate over a list.\n",
    "import imageio  # Creates images/video from data.\n",
    "import matplotlib # Plotting package\n",
    "import matplotlib.pyplot as plt  # Plotting package with additional funcitons.\n",
    "import numpy as np  # Scientific computing using arrays.\n",
    "import scipy  # Optimized for linear algebra, signal processing.\n",
    "import skimage  # Image analysis (scikit).\n",
    "import torch  # PyTorch library for neural network applications.\n",
    "import trackpy as tp  # Particle tracking package (Crocker & Grier method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d5251",
   "metadata": {},
   "source": [
    "Load a set of functions written for this notebook from a utilities file. Check the `utils.py` file for the thorough documentation of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions and utilities for dataset generation and visualization.\n",
    "# Sys append a folder to the path.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import all the functions contained in the utilities file utils.py.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b6d4d-cc42-4522-8299-8b2cc8d73d9f",
   "metadata": {},
   "source": [
    "### 2. Visualization of an experiment\n",
    "Load a TIF image contained in a folder called `images`. Visualize the image using SciKit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4594775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and image file name.\n",
    "image_folder = \"images\"\n",
    "image_file_name = \"JT008_AFM_15.tif\"\n",
    "\n",
    "# Construct the full path.\n",
    "image_path = os.path.join(image_folder, image_file_name)\n",
    "\n",
    "# Create an instance of the loaded file using SciKit.\n",
    "image = skimage.io.imread(image_path)\n",
    "\n",
    "# Crop the image to a square.\n",
    "#image = image[:1024, :1024]\n",
    "image = image[:640, :640]\n",
    "\n",
    "# Print the name of the selected file.\n",
    "print(\"Selected file:\", image_path)\n",
    "\n",
    "# Open a figure instance.\n",
    "plt.figure()\n",
    "\n",
    "# Plot the image in grayscale.\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Experimental image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the image size in pixels.\n",
    "print(f\"Size of imported image in pixels: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c864a5-b676-4333-ba03-30d727494cc4",
   "metadata": {},
   "source": [
    "Manually select and display a single object by specifying its coordinates (x,y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b4256-0818-44ab-8f66-c3e3f07e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size to zoom in an individual particle.\n",
    "width = 22 \n",
    "\n",
    "# Position of the window.\n",
    "x = 329 - width // 2 \n",
    "y = 212 - width // 2\n",
    "\n",
    "# Select a crop as a subset of the entire image.\n",
    "crop = image[x:x+width, y:y + width]\n",
    "\n",
    "# Initialize figure instance.\n",
    "fig = plt.figure()\n",
    "\n",
    "# Draw a red rectangle around the crop.\n",
    "fig.add_subplot(111)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Experimental image\", size=13)\n",
    "plt.plot([y, y+width, y+width, y, y], [x, x, x+width, x+width, x], 'r-')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot the rectangle on the top right corner.\n",
    "fig.add_subplot(555)\n",
    "plt.imshow(crop, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aeb622-c5d3-42f5-8668-419647f7be24",
   "metadata": {},
   "source": [
    "### 3. Use DeepTrack to simulate a realistic image. \n",
    "DeepTrack can be used to simulate physically realistic scenarios. Simulations provide ground-truth to test the performance of different methods and can be used to train neural network models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d7705-e251-4cee-ae05-1710ada00300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as when selecting a single object.\n",
    "image_size = width  \n",
    "\n",
    "particle_radius = 950 # In nm.\n",
    "\n",
    "# Define central spherical scatterer.\n",
    "sphere = dt.Sphere(\n",
    "    position=0.5 * np.array([image_size, image_size]),\n",
    "    z=-100 * dt.units.nm, # Particle out of focus.\n",
    "    radius= particle_radius * dt.units.nm,  # Radius in nanometers.\n",
    "    intensity= 900,  # Field magnitude squared.\n",
    ")\n",
    "\n",
    "# Simulate the properties of the fluorescence microscope.\n",
    "optics = dt.Fluorescence(\n",
    "    NA=0.4,  # Numerical aperture.\n",
    "    wavelength=638. * dt.units.nm, \n",
    "    refractive_index_medium=1.33, \n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=100 * dt.units.nm, # Camera resolution or effective resolution.\n",
    "    )\n",
    "\n",
    "# Apply transformations to crop.\n",
    "sim_crop = (\n",
    "    optics(sphere)\n",
    "    >> dt.NormalizeMinMax(0.0, 1.0)\n",
    "    )\n",
    "\n",
    "# Turn the crop into a NumPy array.\n",
    "simulated_crop = np.squeeze(sim_crop())\n",
    "\n",
    "# Plot the simulated and experimental crops.\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Simulated crop.\n",
    "plot = axes[0].imshow(simulated_crop, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Simulated Crop\")\n",
    "\n",
    "\n",
    "# Experimental crop.\n",
    "axes[1].imshow(crop, cmap=\"gray\")  \n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Experimental Crop\")\n",
    "\n",
    "# Adjust layout and show plot.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display the intensity profile of the simulated experimental crops.\n",
    "simulated_crop_intensity = simulated_crop[image_size // 2, :]\n",
    "experimental_crop_intensity = crop[image_size // 2, :]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(simulated_crop_intensity, label=\"Simulated Crop\", color=\"blue\")\n",
    "ax.plot(experimental_crop_intensity/experimental_crop_intensity.max(), label=\"Experimental Crop\", color=\"orange\")  # Fixed method call to 'max()'\n",
    "ax.set_xlabel(\"Pixel\")\n",
    "ax.set_ylabel(\"Intensity\")\n",
    "ax.set_title(\"Intensity Profile Comparison\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b890387-3fd7-4e6c-9651-76d067b9efae",
   "metadata": {},
   "source": [
    "### Generate a simulated image to test different detection methods\n",
    "We generate an image of non-overlapping, spherical particles. First, their coordinates are generated, which are referred to as ground truth positions. Then, we use DeepTrack to place optically realistic particles on such positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d981d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the simulation.\n",
    "image_size = 256\n",
    "N_particles = 128\n",
    "particle_radius = 900  # Particle radius in nm (100 nm = 1 px).\n",
    "particle_radius_px = particle_radius / 100 # In pixels\n",
    "\n",
    "# Dictionaries for shell and core properties. Dimensions are set with lambda\n",
    "# functions to introduce variety to the dataset.\n",
    "sphere_properties = {\n",
    "    \"radius\": lambda: np.random.uniform(0.85, 1.05) * particle_radius * dt.units.nm,\n",
    "    \"intensity\": lambda: np.random.uniform(0.7, 1.3) * 900,\n",
    "    }\n",
    "\n",
    "### CM: Isn't the same as above? \n",
    "# # Set the optical properties of the microscope.  \n",
    "# optics_properties = dt.Fluorescence(\n",
    "#     NA=0.4,  # Numerical aperture.\n",
    "#     wavelength=638. * dt.units.nm, \n",
    "#     refractive_index_medium=1.33, \n",
    "#     output_region=[0, 0, image_size, image_size],\n",
    "#     magnification=1,\n",
    "#     resolution=100 * dt.units.nm, # Camera resolution or effective resolution.\n",
    "#     )\n",
    "\n",
    "# Generate ground truth positions.\n",
    "gt_pos = utils.generate_centroids(\n",
    "    num_particles=N_particles,\n",
    "    image_size=image_size,\n",
    "    particle_radius=particle_radius, \n",
    "    )\n",
    "\n",
    "# Simulate image.\n",
    "simulated_image = utils.transform_to_video(\n",
    "    gt_pos,\n",
    "    image_size=image_size,\n",
    "    core_particle_props=sphere_properties,\n",
    "    optics_props=optics_properties,\n",
    "    )\n",
    "\n",
    "# Generate the ground truth as Gaussian clouds centered at the position of the\n",
    "# spheres. The standard deviation of the Gaussians are set to be proportional \n",
    "# to the radii of the corresponding spherical particles.\n",
    "ground_truth_cloud_size = particle_radius / 3\n",
    "\n",
    "# Generate ground truth map using the positions, radii and intensities of the \n",
    "# particles.\n",
    "simulated_map = utils.create_ground_truth_map(\n",
    "    gt_pos, \n",
    "    image_size=image_size, \n",
    "    sigma=ground_truth_cloud_size, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b7299",
   "metadata": {},
   "source": [
    "Visualize the simulated image and the corresponding ground truth map. Each particle has an associated ground truth cloud with a Gaussian profile, whose standard deviation is proportional to the radius of the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subfigure instance to plot the image and the probability map.\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# Instance for the simulated experimental image. \n",
    "img1 = ax[0].imshow(simulated_image, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Simulated image\")  \n",
    "\n",
    "# Instance for the probability map.\n",
    "img2 = ax[1].imshow(simulated_map, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Simulated Probability Map (Gaussian)\") \n",
    "\n",
    "# Adjust layout to avoid overlap.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64f18d-cae8-43b9-9554-ed8037dc6d9b",
   "metadata": {},
   "source": [
    "### 4. Detection and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461c273-1527-402d-b696-d789ac859e69",
   "metadata": {},
   "source": [
    "After generating a realistic simulation of the experiment, we can implement, evaluate and compare different detection algorithms. This is enabled by the fact that the ground truth —i.e., the actual particle positions— is known, allowing for a direct and precise comparison of the detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316cec7-157a-463d-af61-4f21d95e45f2",
   "metadata": {},
   "source": [
    "The following detection methods will be implemented and evaluated with respect to the ground truth:\n",
    "\n",
    "1. Thresholding (using scikit regionprops)\n",
    "2. Crocker and Grier (using trackpy)\n",
    "3. UNet convolutional neural network (using DeepTrack)\n",
    "4. Lodestar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72d222-07b8-4001-b5ac-ea169cfb4785",
   "metadata": {},
   "source": [
    "## Method 1: Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d951f47-8fa3-4e5b-b03e-f080f6dbe0fb",
   "metadata": {},
   "source": [
    "In a grayscale image, each point has an intensity value that, when normalized, ranges from 0 to 1, where 1 represents the highest signal intensity arising from particles and values approaching 0 indicate the background in the case of high signal-to-noise ratio (SNR). Thresholding creates a binary mask of the image, setting intensity values to 1 for particles and 0 elsewhere. This is performed by setting a user-defined intensity threshold. Any value above this quantity is round up to 1, else, round down to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6564822",
   "metadata": {},
   "source": [
    "Optional: A straightforward method for determining an unbiased intensity threshold involves computing the mean intensity $I_{mean}$ and standard deviation $\\sigma_{I}$ of the intensity of a normalized image. We propose an empirical threshold defined as $I_{thresh} = I_{mean} +/- \\alpha * \\sigma_{I}$, where $\\alpha$ tipically ranges from, 0 to 3. \n",
    "\n",
    "This threshold is determined by statistically significant deviations from the mean.\n",
    "The sign + or - is chosen based on whether the particles appear brighter or darker than the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a34bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Gaussian filter to the image to smooth out noise.\n",
    "filtered_image_sim = scipy.ndimage.gaussian_filter(simulated_image, sigma=1.0)\n",
    "\n",
    "# Calculate the mean and standard deviation of the simulated image.\n",
    "# Flatten the images to 1D for histograms\n",
    "flattened_image_filtered = filtered_image_sim.flatten()\n",
    "flattened_image_simulated = simulated_image.flatten()\n",
    "\n",
    "# Compute statistics for simulated image with and without Gaussian filter.\n",
    "mean_intensity_filtered = np.median(flattened_image_filtered)\n",
    "std_intensity_filtered = np.std(flattened_image_filtered)\n",
    "mean_intensity_simulated = np.mean(flattened_image_simulated)\n",
    "std_intensity_simulated = np.std(flattened_image_simulated)\n",
    "\n",
    "# We define the threshold in terms of mean intensity and standard deviation.\n",
    "intensity_threshold = mean_intensity_filtered + 0.75 * std_intensity_filtered\n",
    "\n",
    "print(f\"Intensity threshold: {intensity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd353e33",
   "metadata": {},
   "source": [
    "Plot the histogram of intensities of the simulated image with and without Gaussian filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# Histogram for simulated image\n",
    "axes[0].hist(flattened_image_simulated, bins=256, range=(0, 1), color='gray', alpha=0.75, density=True)\n",
    "axes[0].axvline(mean_intensity_simulated, color='red', linestyle='dashed', linewidth=1.5, label=\"Mean\")\n",
    "axes[0].axvline(mean_intensity_simulated - std_intensity_simulated, color='blue', linestyle='dashed', linewidth=1, label=\"Mean - Std. Dev.\")\n",
    "axes[0].axvline(mean_intensity_simulated + std_intensity_simulated, color='blue', linestyle='dashed', linewidth=1, label=\"Mean + Std. Dev.\")\n",
    "axes[0].set_title(\"Simulated Image\", fontsize=14)\n",
    "axes[0].set_yticklabels([])  # Remove y labels\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Histogram for filtered image\n",
    "axes[1].hist(flattened_image_filtered, bins=256, range=(0, 1), color='gray', alpha=0.75)\n",
    "axes[1].axvline(mean_intensity_filtered, color='red', linestyle='dashed', linewidth=1.5, label=\"Mean\")\n",
    "axes[1].axvline(mean_intensity_filtered - std_intensity_filtered, color='blue', linestyle='dashed', linewidth=1, label=\"Mean - Std. Dev.\")\n",
    "axes[1].axvline(mean_intensity_filtered + std_intensity_filtered, color='blue', linestyle='dashed', linewidth=1, label=\"Mean + Std. Dev.\")\n",
    "axes[1].set_title(\"Simulated image + Gaussian filter\", fontsize=14)\n",
    "axes[1].set_yticklabels([])  # Remove y labels\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Set common x and y labels\n",
    "fig.supxlabel(\"Normalized Intensity\", fontsize=12)\n",
    "fig.supylabel(\"Normalized Frequency [A.U.]\", fontsize=12)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d1cbc-2e83-4087-9e45-63bdf1aab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the binary mask by applying the threshold.\n",
    "simulated_mask = simulated_image > intensity_threshold\n",
    "\n",
    "# Ensure the mask is a 2D array.\n",
    "simulated_mask = np.squeeze(simulated_mask)\n",
    "\n",
    "# Plot the image with the calculated mask.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=simulated_image, \n",
    "    mask=simulated_mask, \n",
    "    title=\"Mask using threshold method\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48868ccd-04bd-4699-9bca-c682dbcf8653",
   "metadata": {},
   "source": [
    "### Determine the centroids from the mask using Scikit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de013b1-04bc-4361-a73a-3ae0b4d20fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to extract positions from mask.\n",
    "detections_M1 = utils.mask_to_positions(simulated_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8322201",
   "metadata": {},
   "source": [
    "### Plot the prediction of the centroids from mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1224ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plotting function to compare the detection with the ground truth.\n",
    "utils.plot_predicted_positions(image=simulated_image, \n",
    "                         predicted_positions=detections_M1, \n",
    "                         ground_truth_positions=gt_pos, \n",
    "                         title=\"Method 1: Thresholding\"\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6739-64ed-4946-bbe4-f7ed428ad22f",
   "metadata": {},
   "source": [
    "### Apply the function to evaluate performance the thresholding method\n",
    "Evaluation metrics are printed: True Positives (TP), False Positives (FP), False Negatives (FN), F1 score (F1) and Mean Square Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e920e9-2229-44db-aa77-fcc393d31dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics can be retrieved if returned outputs are asigned to a variable:\n",
    "# TP, FP, FN, F1, RMSE = utils.evaluate_locs(detections_M1, gt_pos, distance_th=particle_radius).\n",
    "\n",
    "utils.evaluate_locs(detections_M1, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9781519-d652-4bac-b831-026038460c42",
   "metadata": {},
   "source": [
    "#### Bonus: Use radial symmetry to refine localization\n",
    "A function is defined in the utility file to exploit the radial symmetry of regions of interest located at the surroundings of previously calculated particle centers. This function works for spherical particles, where the radial symmetry assumption is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0636df",
   "metadata": {},
   "source": [
    "Combine radial symmetry with thresholding method for a better estimation of the centroid location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the location of the centroid of a masked particle optical signal.\n",
    "detections_M1_ref = utils.locate_particle_centers(\n",
    "    detections_M1, \n",
    "    simulated_image, \n",
    "    estimated_radius=particle_radius\n",
    ")\n",
    "\n",
    "# Use the plotting function to compare the detection with the ground truth.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image, \n",
    "    predicted_positions=detections_M1_ref, \n",
    "    ground_truth_positions=gt_pos, \n",
    "    title=\"Method 1: Thresholding with radial symmetry refinement\"\n",
    ")\n",
    "\n",
    "# Evaluate the performance.\n",
    "utils.evaluate_locs(detections_M1_ref, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b575722-b518-4d85-871a-06cb635b9e8e",
   "metadata": {},
   "source": [
    "### Apply the threshold method to loaded experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a359c4-acba-4b2e-bf05-cd8d8a9184f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the experimental image to have (0,1) intensity values.\n",
    "image = utils.normalize_min_max(image)\n",
    "\n",
    "# Intensity threshold for the experimental image.\n",
    "intensity_threshold_exp = 0.65\n",
    "\n",
    "# Create the binary mask by applying the threshold.\n",
    "mask_exp = image > intensity_threshold_exp\n",
    "\n",
    "# Get predicted positions from the mask.\n",
    "detections_M1_exp = utils.mask_to_positions(mask_exp)\n",
    "\n",
    "detections_M1_exp_ref = utils.locate_particle_centers(\n",
    "    detections_M1_exp,\n",
    "    image,\n",
    "    estimated_radius=particle_radius,\n",
    ")\n",
    "\n",
    "utils.plot_predicted_positions(\n",
    "    image=image,\n",
    "    predicted_positions=detections_M1_exp,\n",
    "    title=\"Method 1 applied to experimental image\",\n",
    ")\n",
    "\n",
    "# Plot the experimental image and the mask.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=image, \n",
    "    mask=mask_exp, \n",
    "    title=\"Mask of experimental image\",\n",
    ")\n",
    "\n",
    "# Print the number of detections.\n",
    "print(f\"Found {len(detections_M1_exp[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bfb12-bce1-4be6-894f-b98482c83779",
   "metadata": {},
   "source": [
    "## Method 2: Crocker and Grier (using Trackpy)\n",
    "The Crocker and Grier method is a built-in function in Trackpy. The only user input parameter is the estimated diameter, which should be an odd number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3734b",
   "metadata": {},
   "source": [
    "#### Extract the diameter of a particle from the experimental image by clicking on both poles of a sphere.\n",
    "Note that there is variability in particle size, so this measurement should be seen as an educated starting point, which is easily refined a posteriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44984328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib interactive backend .\n",
    "%matplotlib widget\n",
    "utils.interactive_ruler(simulated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and tune the diameter of particles in pixels, ensuring it is an\n",
    "# odd number. Adjust if necessary.\n",
    "estimated_diameter = 17\n",
    "\n",
    "# Use the locate built-in function from TrackPy.\n",
    "    # Output is type DataFrame([x, y, mass, size, ecc, signal, raw_mass]).\n",
    "localizations_dataframe = tp.locate(\n",
    "    simulated_image, \n",
    "    diameter=estimated_diameter,\n",
    "    noise_size=4, # Setting for denoising image.\n",
    "    )\n",
    "\n",
    "# Extract the predicted positions from DataFrame[\"x\",\"y\"] into a NumPy array.\n",
    "detections_M2 = np.array(localizations_dataframe)[:,:2]\n",
    "\n",
    "# Plot the prediction and the ground truth.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_M2,\n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Method 2. Crocker and Grier\",\n",
    ")\n",
    "\n",
    "# Evaluate the performance of tp.locate.\n",
    "utils.evaluate_locs(\n",
    "    detections_M2,\n",
    "    gt_pos,\n",
    "    distance_th=particle_radius,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d5b5d-602f-4c59-a360-ac4f3a6a8dea",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5797d8-2786-460a-86c9-de8ab51645b9",
   "metadata": {},
   "source": [
    "## Apply to experimental image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3838216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib interactive backend .\n",
    "%matplotlib widget\n",
    "utils.interactive_ruler(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d9ba9-c1f0-424c-838c-ae391acb2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Trackpy.locate to experimental data file 'image'.\n",
    "localizations_dataframe_exp = tp.locate(\n",
    "    image, \n",
    "    diameter=estimated_diameter - 2, # Tune the estimated diameter at will. \n",
    "    invert=False,\n",
    "    preprocess=True,\n",
    ")\n",
    "\n",
    "# Extract the predicted positions from DataFrame[\"x\",\"y\"] into a NumPy array.\n",
    "detections_M2_exp = np.array(localizations_dataframe_exp)[:,:2]\n",
    "\n",
    "# Plot the experimental image with predicted positions overlaid.\n",
    "utils.plot_predicted_positions(\n",
    "    image=image,\n",
    "    predicted_positions=detections_M2_exp,\n",
    "    title=\"Method 2 applied to experimental image\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(detections_M2_exp[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6cb5f-f4d9-45f8-b051-92f8ec153ccb",
   "metadata": {},
   "source": [
    "## Method 3: UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8554a3-2703-457d-b1f3-ff5045ea5d68",
   "metadata": {},
   "source": [
    "U-Net is a convolutional neural network (CNN), a type of deep learning model, commonly used for image segmentation tasks. It assigns a class label to each pixel in an image, allowing for precise differentiation between the background and objects of interest, from particles to complex cells or anatomical structures, depending on the training. The name \"U-Net\" comes from its U-shaped architecture, where the encoder (downsampling path) and decoder (upsampling path) are symmetric. The network's bottleneck, located at the center of the U, captures high-level features at reduced spatial resolution, while skip connections preserve fine-grained spatial details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75488fa-f1a5-4252-9f06-5df7609af1e7",
   "metadata": {},
   "source": [
    "### Create a training dataset\n",
    "Simulated images of particles are generated using DeepTrack, along with their corresponding probability maps, which serve as the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90b159",
   "metadata": {},
   "source": [
    "Load a dataset if available, otherwise, generate it according to the size of the sample and the image size defined by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762c048-9ed8-45c8-bd3b-758c21329245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples, image size, and particles.\n",
    "num_samples = 128\n",
    "image_size = 64\n",
    "max_num_particles = 10\n",
    "force_simulation = True  # Flag to force simulation even if data exists.\n",
    "\n",
    "# # Radius and intensity set to zero means no shell.\n",
    "# shell_particle_properties = {\n",
    "#     \"radius\": 0 * dt.units.nm,\n",
    "#     \"intensity\": 0,\n",
    "# }\n",
    "\n",
    "# Simulate the properties of the fluorescence microscope.\n",
    "optics_properties = dt.Fluorescence(\n",
    "    NA=1.4,  # Numerical aperture.\n",
    "    wavelength=638. * dt.units.nm, \n",
    "    refractive_index_medium=1.33, \n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=100 * dt.units.nm, # Camera resolution or effective resolution.\n",
    "    )\n",
    "\n",
    "# Create path to store training dataset.\n",
    "folder_name = \"UNet\"\n",
    "training_dataset_filename = \"UNet_training_dataset_spheres.npz\"\n",
    "training_dataset_folder = folder_name + \"/\" + \"training_data\"\n",
    "training_dataset_filepath = (\n",
    "    training_dataset_folder + \"/\" + training_dataset_filename\n",
    "    )\n",
    "\n",
    "# Create the enclosing directory if not existent already.\n",
    "if not os.path.exists(training_dataset_folder):\n",
    "    os.makedirs(training_dataset_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Try to load preexisting data, if not available or forced, raise the\n",
    "# exception error to generate new data.\n",
    "try:\n",
    "    if force_simulation: \n",
    "        # Raise the exception error if simulation is forced.\n",
    "        raise FileNotFoundError(\"Forced simulation by user request.\")\n",
    "    \n",
    "    if not os.path.isfile(training_dataset_filepath):\n",
    "        # If file is not found, start training.    \n",
    "        raise FileNotFoundError(\n",
    "            \"Training dataset file not found. Starting simulation.\"\n",
    "        )\n",
    "    \n",
    "    # Load existing data\n",
    "    data = np.load(training_dataset_filepath)\n",
    "    print(f\"Loaded file: {training_dataset_filepath}\")\n",
    "    images = data['images']\n",
    "    maps = data['maps']\n",
    "    Nsamples = len(images)\n",
    "        \n",
    "# Handle the case of either file not found or forced training.\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # Perform simulation if file not found or forced.\n",
    "    images, maps = utils.generate_particle_dataset(\n",
    "        num_samples,\n",
    "        image_size,\n",
    "        max_num_particles,\n",
    "        sphere_properties,\n",
    "        optics_properties=optics_properties,\n",
    "        )\n",
    "    \n",
    "    # Save the simulated training dataset.\n",
    "    np.savez(training_dataset_filepath, images=images, maps=maps)\n",
    "    print(f\"Training dataset saved in: {training_dataset_filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453aaeb",
   "metadata": {},
   "source": [
    "Check dimensions of training data. The format to train a U-Net model with Deeplay should be (N,X,Y,C), where:\\\n",
    "\\\n",
    "N: Number of samples (simulated images).\\\n",
    "X: Intensity in the X position.\\\n",
    "Y: Intensity in the Y position.\\\n",
    "C: Color channel (1 for grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2234b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image dimensions: {images.shape}\")\n",
    "print(f\"Maps dimensions: {maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fd2d5",
   "metadata": {},
   "source": [
    "### Visualize the simulated training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image and its corresponding probability maps and mask to show.\n",
    "selected_image_index = np.random.randint(0, len(images))\n",
    "\n",
    "# Extract the image and probability map from 4D arrays.\n",
    "selected_image = np.squeeze(images[selected_image_index])\n",
    "simulated_probability_map = np.squeeze(maps[selected_image_index])\n",
    "\n",
    "# # Plot the image as the first subplot.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=selected_image,\n",
    "    ground_truth_map=simulated_probability_map,\n",
    "    title=f\"Training dataset element: {selected_image_index + 1}/{len(images)}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3bb9d-a80a-478a-8566-5bbab51333b3",
   "metadata": {},
   "source": [
    "## Define a U-Net model using Deeplay\n",
    "The input and output channels of the 2D U-Net are equal to 1, since for the moment we are only analyzing one single color channel corresponding to light intensity in grayscale. We want the U-Net model to be able to recognize a probability map of particles from an experimental image. The single-channel output will then be the spatial probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6871e28",
   "metadata": {},
   "source": [
    "The U-Net architecture need to be deep enough in order to have a good (reduced) representation of the input image. In order to set the number of convolutional layers N, we use the heuristic argument that N layers, provided with a (default) kernel size of 3x3, enables a (3x3)^N window to look for features in an image. Three layers will enable a 27x27 window, suitable to train a detector of particles with diameters up to 27 pixel units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb159ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output channels are set to 1, since both images and probability\n",
    "# maps only have one color channel.\n",
    "unet = dl.UNet2d(\n",
    "    in_channels=1, \n",
    "    channels=[16, 32, 64], \n",
    "    out_channels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e56ba9",
   "metadata": {},
   "source": [
    "### Compile the model for the task of regressing the masks from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c64c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A U-Net regressor is defined using a deeplay model. The loss function is Mean\n",
    "# Squared Error Loss, since the output is a continuous intensity map.\n",
    "unet_reg = dl.Regressor(\n",
    "    model=unet, \n",
    "    loss=torch.nn.MSELoss(), \n",
    "    optimizer=dl.Adam(lr=1e-3),\n",
    "    ).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5d9e0",
   "metadata": {},
   "source": [
    "### Define the training pipeline using Deeptrack to generate the training dataset\n",
    "The training pipeline is composed of the simulated images and the probability maps, together with instructions of value normalization and a selector of images, all of which are instances of Deeplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image selector with a random picker. This is performed in order to properly \n",
    "# link an element in maps array with its corresponding element in images \n",
    "# array.\n",
    "selector = dt.Lambda(\n",
    "    lambda i: lambda x: x[i], i=lambda l: np.random.randint(l), l=len(images)\n",
    "    )\n",
    "\n",
    "# Apply augmentations of added Gaussian noise only to images.\n",
    "images_augmentations = (\n",
    "    dt.Value(images)\n",
    "    >> dt.Gaussian(0.0, 0.015)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), \n",
    "        lambda: np.random.uniform(0.9, 1.0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Apply normalization to ground truth map.\n",
    "maps = dt.Value(maps) >> dt.NormalizeMinMax(0.0, 1.0)\n",
    "\n",
    "# Training pipeline consists of two paired lists: images and maps. \n",
    "# Images are augmented and normalized, while maps are only normalized.\n",
    "# The selector randomly selects an image and its corresponding ground truth map.\n",
    "# The pipeline is then passed to the deeplay model.\n",
    "pipeline = (images_augmentations & maps) >> selector\n",
    "\n",
    "# Create the training dataset instance in PyTorch\n",
    "train_dataset = dt.pytorch.Dataset(pipeline, length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56847fc3",
   "metadata": {},
   "source": [
    "Check one augmented crop at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe52aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "sanity_check_pipeline_augmentation = np.squeeze(pipeline.update().resolve())\n",
    "sanity_check_image_augmentation = sanity_check_pipeline_augmentation[0]\n",
    "sanity_check_map_augmentation = sanity_check_pipeline_augmentation[1]\n",
    "\n",
    "# # Plot the image as the first subplot\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=sanity_check_image_augmentation,\n",
    "    ground_truth_map=sanity_check_map_augmentation,\n",
    "    title=f\"Random augmentation from training pipeline\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797d694-9bab-4505-93ab-4d36d4174f30",
   "metadata": {},
   "source": [
    "### Train the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638011-81a0-482b-a7eb-4aeb776312d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for training.\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize UNet trainer with automatic accelerator (e.g., CUDA if available).\n",
    "unet_trainer = dl.Trainer(max_epochs=64, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34721c",
   "metadata": {},
   "source": [
    "Tries to load weights if existent. Else, start training and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force training if desired.\n",
    "force_training = True\n",
    "\n",
    "# Define the file paths for the model weights.\n",
    "unet_path = \"UNet_model_spheres\"\n",
    "unet_reg_path = \"UNet_reg_spheres\"\n",
    "\n",
    "# Define the paths to store files inside the U-Net folder.\n",
    "unet_path =  folder_name + \"/\" + unet_path\n",
    "unet_reg_path = folder_name + \"/\" + unet_reg_path\n",
    "\n",
    "# Check if both weight files exist in order to load them.\n",
    "if os.path.exists(unet_path) and os.path.exists(unet_reg_path):\n",
    "    \n",
    "    # Load the dictionaries\n",
    "    unet.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "    unet_reg.load_state_dict(torch.load(unet_reg_path, weights_only=True))\n",
    "    print(f\"Model weights already existent. \\\n",
    "        Loaded successfully from working directory.\")\n",
    "\n",
    "# Starts training if forced or if unet weights are not found in the path.\n",
    "if force_training or not os.path.exists(unet_path):\n",
    "    \n",
    "    # Create the enclosing directory if not existent already.\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    \n",
    "    # Start training with the regressor and the data loader.\n",
    "    unet_trainer.fit(unet_reg, train_loader)\n",
    "    \n",
    "    # Save the pre-trained weights of the U-Net model as Python dictionaries.\n",
    "    torch.save(unet.state_dict(), unet_path)\n",
    "    torch.save(unet_reg.state_dict(), unet_reg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823a1c0",
   "metadata": {},
   "source": [
    "### Apply the trained U-Net to the simulated image\n",
    "Select a new image that has not been used for the training to infere the probability map, based on the learning acquired by the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a79a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image to apply the learning by the U-Net.\n",
    "image_of_particles = np.array(simulated_image).astype(np.float32)\n",
    "print(f\" Shape of selected image: {image_of_particles.shape} \")\n",
    "\n",
    "# Normalize intensity of image to (0,1).\n",
    "image_of_particles = utils.normalize_min_max(image_of_particles)\n",
    "\n",
    "# Create a new axis and rearrange to (N,X,Y,C) format.\n",
    "# In this case, it should be (1, 1, 256, 256).\n",
    "image_of_particles = np.expand_dims(image_of_particles, axis=0)\n",
    "image_of_particles = image_of_particles.transpose(0, 3, 1, 2)\n",
    "print(f\" Shape of selected image after transposing to match the (N, C, X, Y): {image_of_particles.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f834b6d",
   "metadata": {},
   "source": [
    "Convert the selected image to a PyTorch tensor to infer the probability map of the particles using the learning of the trained U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to analyze into a PyTorch tensor.\n",
    "image_of_particles_tensor = torch.from_numpy(image_of_particles)\n",
    "\n",
    "# Apply the UNet to the loaded image.\n",
    "pred_maps_tensor = unet_reg(image_of_particles_tensor)\n",
    "\n",
    "# Convert to NumPy array.\n",
    "pred_maps_array = pred_maps_tensor[0,0,:,:].detach().numpy()\n",
    "\n",
    "# Normalize intensity.\n",
    "pred_maps_array = utils.normalize_min_max(pred_maps_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2281d2",
   "metadata": {},
   "source": [
    "Apply the mask method to the predicted probability map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87363d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply thresholding to the predicted ground truth map.\n",
    "pred_mask_unet = pred_maps_array > 0.25\n",
    "\n",
    "# Convert the masked ground truth map to positions.\n",
    "detections_unet = utils.mask_to_positions(pred_mask_unet)\n",
    "\n",
    "# Show the predicted posiions overlaid with the ground truth on the simulated\n",
    "# image.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_unet, \n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Prediction from U-Net\"\n",
    "    )\n",
    "\n",
    "# Measure the performance.\n",
    "utils.evaluate_locs(detections_unet, gt_pos, distance_th=particle_radius_px)\n",
    "\n",
    "#Plot the predicted ground truth map and its masked version.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    mask=pred_mask_unet,\n",
    "    ground_truth_map=pred_maps_array,\n",
    "    title=\"U-Net prediction on simulated data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2d1d0",
   "metadata": {},
   "source": [
    "### Apply the trained U-Net to the experimental image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d468f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize intensity of image to (0,1).\n",
    "image_of_particles_exp = utils.normalize_min_max(image)\n",
    "\n",
    "# Reshape the array to match the (N, C, X, Y) format.\n",
    "image_of_particles_exp = image_of_particles_exp[np.newaxis, np.newaxis, ...]\n",
    "\n",
    "# Ensure that the image is a numpy array.\n",
    "image_of_particles_exp = np.array(image_of_particles_exp).astype(np.float32) \n",
    "\n",
    "# Convert the image to analyze into a PyTorch tensor.\n",
    "image_of_particles_exp_tensor = torch.from_numpy(image_of_particles_exp)\n",
    "\n",
    "# Apply the UNet to the loaded image.\n",
    "pred_maps_unet_exp = unet_reg(image_of_particles_exp_tensor)\n",
    "\n",
    "# Convert the prediction to a NumPy array for easy plotting.\n",
    "pred_maps_unet_exp_array = pred_maps_unet_exp[0, 0, :, :].detach().numpy()\n",
    "\n",
    "# Normalize intensity to (0,1).\n",
    "pred_maps_unet_exp_array = utils.normalize_min_max(pred_maps_unet_exp_array)\n",
    "\n",
    "# Calculate a mask from the ground truth map with thresholding method.\n",
    "pred_mask_unet_exp = pred_maps_unet_exp_array > 0.2\n",
    "\n",
    "# Extract the positions from the mask.\n",
    "detections_unet_exp = utils.mask_to_positions(pred_mask_unet_exp)\n",
    "\n",
    "# Plot the experimental image with the positions predicted by U-Net.\n",
    "utils.plot_predicted_positions(image=image, \n",
    "                         predicted_positions=detections_unet_exp,\n",
    "                         title=\"Positions predicted by U-Net. Experimental image\",\n",
    "                         )\n",
    "\n",
    "#Plot the predicted ground truth map and its masked version.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    mask=pred_mask_unet_exp,\n",
    "    ground_truth_map=pred_maps_unet_exp_array,\n",
    "    title=\"U-Net prediction on experimental image\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dccf5-8255-4032-bbe0-4a3ae9685d06",
   "metadata": {},
   "source": [
    "## Method 4: LodeSTAR\n",
    "LodeSTAR is a self-supervised object detection method that can be trained using a single input image.\\\n",
    "LodeSTAR will be trained with simulated crops to measure its performance on simulated data and then experimental data. The same idea will be applied to train LodeSTAR with experimental crops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70f00e",
   "metadata": {},
   "source": [
    "### Initiate a training pipeline with only one simulated crop.\n",
    "We can apply augmentations to the same crop to increase the diversity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples, image size, and particles.\n",
    "num_samples = 128\n",
    "image_size = 30\n",
    "max_num_particles = 1\n",
    "force_simulation = False  # Flag to force simulation even if data exists.\n",
    "\n",
    "# Create path to store training dataset.\n",
    "folder_name = \"LodeSTAR\"\n",
    "training_dataset_filename = \"LodeSTAR_training_dataset_ellipsoids.npz\"\n",
    "training_dataset_folder = folder_name + \"/\" + \"training_data\"\n",
    "training_dataset_filepath = (\n",
    "    training_dataset_folder + \"/\" + training_dataset_filename\n",
    "    )\n",
    "\n",
    "# Create the enclosing directory if not existent already.\n",
    "os.makedirs(training_dataset_folder, exist_ok=True)\n",
    "\n",
    "# Dictionaries for shell and core properties. Dimensions are set with lambda\n",
    "# functions to introduce a bounded level of randomness to the dataset. \n",
    "core_particle_properties = {\n",
    "    \"radius\": lambda: np.random.uniform(0.7, 1.2) * particle_radius * dt.units.nm,\n",
    "    \"intensity\": lambda: np.random.uniform(0.7, 1.3) * 900,\n",
    "    }\n",
    "\n",
    "# Set the optical properties of the microscope.\n",
    "optics_properties = dt.Fluorescence(\n",
    "    NA=0.4,  # Numerical aperture\n",
    "    wavelength=638.0 * dt.units.nm,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=100 * dt.units.nm,  # Camera resolution or effective resolution.\n",
    "    )\n",
    "\n",
    "# Try to load preexisting simulated data; if not available or forced, raise the\n",
    "# exception error to generate new data.\n",
    "try:\n",
    "    if force_simulation: \n",
    "        # Raise the exception error if simulation is forced.\n",
    "        raise FileNotFoundError(\"Forced simulation by user request.\")\n",
    "    \n",
    "    if not os.path.isfile(training_dataset_filepath):\n",
    "        # If file is not found, start training.    \n",
    "        raise FileNotFoundError(\"Training dataset file not found. Starting simulation.\")\n",
    "    \n",
    "    # Load existing data\n",
    "    data = np.load(training_dataset_filepath)\n",
    "    crop_images = data[\"crop_images\"]\n",
    "    crop_maps = data[\"crop_maps\"]\n",
    "    Nsamples = len(crop_images)  # Update number of samples.\n",
    "    print(f\"Loaded file: {training_dataset_filepath}\")\n",
    "        \n",
    "# Handle the case of either file not found or forced training.\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # Perform simulation if file not found or forced.\n",
    "    crop_images, crop_maps = utils.generate_particle_dataset(\n",
    "        num_samples,\n",
    "        image_size,\n",
    "        max_num_particles,\n",
    "        core_particle_properties,\n",
    "        shell_particle_dict=None,\n",
    "        optics_properties=optics_properties\n",
    "        )\n",
    "\n",
    "    # Save the simulated training dataset.\n",
    "    np.savez(\n",
    "        training_dataset_filepath, \n",
    "        crop_images=crop_images, \n",
    "        crop_maps=crop_maps\n",
    "        )\n",
    "    print(\"Generated and saved new dataset.\")\n",
    "    \n",
    "    # Plot up to 18 simulated crops.\n",
    "utils.plot_crops(np.squeeze(crop_images), title=\"Simulated crops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training pipeline with additional settings, such as multiplication\n",
    "# of global intensity and Gaussian noise.\n",
    "selector = dt.Lambda(\n",
    "    lambda i: lambda x: x[i], i=lambda l: np.random.randint(l), l=len(crop_images)\n",
    "    )\n",
    "\n",
    "# Define a training pipeline with augmentations. \n",
    "training_pipeline = (\n",
    "    dt.Value(crop_images)\n",
    "    #>> dt.Gaussian(0, 0.025)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), np.random.uniform(0.8, 1.0)\n",
    "        )\n",
    "    >> selector\n",
    "    )\n",
    "\n",
    "# Build a training dataset with a pipeline that randomly selects a crop each\n",
    "# time.\n",
    "train_dataset = dt.pytorch.Dataset(\n",
    "    training_pipeline,\n",
    "    length=256,\n",
    "    )\n",
    "\n",
    "# Dataloader contains the randomized training dataset and additional settings.\n",
    "dataloader = dl.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    )\n",
    "\n",
    "# We can check one of the elements in the training pipeline with augmentations.\n",
    "sanity_check_crop = training_pipeline.update().resolve()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(sanity_check_crop), cmap=\"gray\", aspect=\"equal\")\n",
    "plt.title(\"Random crop from training pipeline\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0e6a4",
   "metadata": {},
   "source": [
    "### Initiate the LodeSTAR model and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the model.\n",
    "lodestar = dl.LodeSTAR(\n",
    "    n_transforms=4, \n",
    "    optimizer=dl.Adam(lr=2e-4), \n",
    "    ).build()\n",
    "\n",
    "# Set up the trainer and specify number of epochs.\n",
    "trainer_lodestar = dl.Trainer(max_epochs=32, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaae69",
   "metadata": {},
   "source": [
    "### Load pre-trained weights or generate new weights.\n",
    "To save time, if you have already run the training you can load pre-existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_training = False\n",
    "\n",
    "# File name of pre-trained weights.\n",
    "lodestar_path = \"lodestar_weights_simulated\"\n",
    "\n",
    "# Define the name of an enclosing folder and add it to the file path.\n",
    "folder_name = \"LodeSTAR\"\n",
    "lodestar_path =  folder_name + \"/\" + lodestar_path\n",
    "\n",
    "#  Check if weights exist or if training will be forced.\n",
    "if not os.path.isfile(lodestar_path) or force_training:\n",
    "    \n",
    "    # Create the enclosing directory if not existent already.\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    #  Start training.\n",
    "    trainer_lodestar.fit(lodestar, dataloader)\n",
    "    torch.save(lodestar.state_dict(),lodestar_path)\n",
    "else:\n",
    "    \n",
    "    #  Load pre-existing weights.\n",
    "    lodestar.load_state_dict(\n",
    "        torch.load(\n",
    "            lodestar_path,\n",
    "            weights_only=True,\n",
    "            )\n",
    "        )\n",
    "    print(\"Loaded preexisting LodeSTAR weights trained on simulated data.\")\n",
    "\n",
    "# Switch the model to evaluation mode.\n",
    "lodestar.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06394103",
   "metadata": {},
   "source": [
    "With the training complete, we are now ready to benchmark the performance by predicting the positions on simulated data, where the ground truth is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for inference.\n",
    "alpha = 0.09\n",
    "beta = 1 - alpha\n",
    "cutoff = 0.001\n",
    "\n",
    "# Extract predictions.\n",
    "prediction = lodestar(image_of_particles_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction.\n",
    "mass_feature = prediction[2]\n",
    "\n",
    "# Infer on the pytorch image.\n",
    "detections_lodestar = lodestar.detect(\n",
    "    image_of_particles_tensor,\n",
    "    alpha = alpha,\n",
    "    beta = beta, \n",
    "    mode = \"constant\",\n",
    "    cutoff = cutoff,\n",
    ")[0]\n",
    "                         \n",
    "print(f\"Found {len(detections_lodestar[:, 1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922d4e2",
   "metadata": {},
   "source": [
    "### Display predictions from the simulated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485466b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(simulated_image), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Simulated image\")\n",
    "\n",
    "#  Subfigure 2, mass feature distribution.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass features predicted by LodeSTAR\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982c9bb",
   "metadata": {},
   "source": [
    "### Overlay detections to simulated image\n",
    "Measure the detection accuracy by contrasting with the ground truth of positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted positions and the ground truth overlaid on the simulated\n",
    "# image.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_lodestar,\n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Method 4: LodeSTAR on simulated image\",\n",
    ")\n",
    "#  Evaluate performance.\n",
    "utils.evaluate_locs(detections_lodestar, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c7ea3",
   "metadata": {},
   "source": [
    "We can apply LodeSTAR trained on simulated data to predict the positions on the experimental image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameters for inference.\n",
    "alpha = 0.09\n",
    "beta = 1 - alpha\n",
    "cutoff = 0.001\n",
    "\n",
    "#  Extract predictions and convert into a tensor\n",
    "prediction_exp = lodestar(image_of_particles_exp_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction\n",
    "mass_feature_exp = prediction_exp[2]\n",
    "\n",
    "#  Infer positions from experimental image.\n",
    "detections_lodestar_experimental = lodestar.detect(\n",
    "    image_of_particles_exp_tensor,\n",
    "    alpha = alpha,\n",
    "    beta = beta, \n",
    "    mode = \"constant\",\n",
    "    cutoff = cutoff,\n",
    "    )[0]\n",
    "                         \n",
    "print(f\"Found {len(detections_lodestar_experimental[:,1])} detections.\")\n",
    "### Display predictions of experimental image\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(image_of_particles_exp), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Experimental image\")\n",
    "\n",
    "#  Subfigure 2, mass features.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature_exp, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass features predicted by LodeSTAR\")\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51698468",
   "metadata": {},
   "source": [
    "Overlay the predicted positions on the experimental image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6075dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predicted_positions(\n",
    "   image=image_of_particles_exp[0, 0, :, :],\n",
    "   predicted_positions= detections_lodestar_experimental,\n",
    "   title=\"Method 4: LodeSTAR on experimental image\"\n",
    ")\n",
    "print(f\"Found {len(detections_lodestar_experimental[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f4fd9",
   "metadata": {},
   "source": [
    "### Training using experimental crops instead of simulated data.\n",
    "Since LodeSTAR is a self-supervised neural network, we can train it with experimental crops instead of simulations. By analyzing the image, we extract the coordinate pairs corresponding to characteristic particles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e012a6",
   "metadata": {},
   "source": [
    "Initiate the training pipeline with experimental crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates from relevant crop samples.\n",
    "xs = [230, 168, 362, 330]\n",
    "ys = [272, 177, 190, 435]\n",
    "number_of_crops = len(xs)\n",
    "\n",
    "# Pre-allocate array to store all the crops.\n",
    "training_images = []\n",
    "for i in range(number_of_crops):\n",
    "    # Go through the locations of the crops in the image.\n",
    "    y_index = ys[i]\n",
    "    x_index = xs[i]\n",
    "    crop_size = 25\n",
    "\n",
    "    # Crop a window in the image.\n",
    "    x0 = x_index\n",
    "    y0 = y_index\n",
    "    training_image = np.array(image[x0:x0 + crop_size, y0: y0 + crop_size])\n",
    "\n",
    "    # Expand dims with np.newaxis and append to list.\n",
    "    training_images.append(training_image[np.newaxis, ...])\n",
    "\n",
    "# Plot up to 18 simulated crops.\n",
    "utils.plot_crops(np.squeeze(training_images), title=\"Experimental crops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965ddf1",
   "metadata": {},
   "source": [
    "Prepare the training pipeline with the selected experimental crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random selector of crops.\n",
    "random_crop = lambda: random.choice(training_images)\n",
    "\n",
    "# Define training pipeline.\n",
    "training_pipeline = (\n",
    "    dt.Value(random_crop)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), np.random.uniform(0.85, 1.0)\n",
    "        )\n",
    "    # >> dt.Gaussian(0.0, 0.01)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Build a training dataset with a pipeline that randomly selects a crop each\n",
    "# time.\n",
    "train_dataset = dt.pytorch.Dataset(\n",
    "    training_pipeline,\n",
    "    length=512,\n",
    ")\n",
    "\n",
    "# Dataloader contains the randomized training dataset and additional settings.\n",
    "dataloader = dl.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "# We can check one of the elements in the training pipeline with augmentations.\n",
    "sanity_check_crop = training_pipeline.update()()\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(sanity_check_crop), cmap=\"gray\", aspect=\"equal\")\n",
    "plt.title(\"Random crop from training pipeline\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881f6b0",
   "metadata": {},
   "source": [
    "### Initiate the LodeSTAR model and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the model as a Deeplay instance.\n",
    "lodestar_exp = dl.LodeSTAR(\n",
    "    n_transforms=8, \n",
    "    optimizer=dl.Adam(lr=1e-3),\n",
    ").build()\n",
    "\n",
    "# Initialize the trainer.\n",
    "trainer_lodestar_exp = dl.Trainer(max_epochs=32)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ab50c",
   "metadata": {},
   "source": [
    "### Load pre-trained weights or generate new weights.\n",
    "To save time, if you have already run the training you can load pre-existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_training = False\n",
    "\n",
    "# File name of pre-trained weights.\n",
    "lodestar_exp_path = \"lodestar_weights_exp\"\n",
    "\n",
    "# Define the name of an enclosing folder and add it to the file path.\n",
    "folder_name = \"LodeSTAR\"\n",
    "lodestar_exp_path =  folder_name + \"/\" + lodestar_exp_path\n",
    "\n",
    "#  Check if weights exist.\n",
    "if not os.path.isfile(lodestar_exp_path) or force_training:\n",
    "    \n",
    "    # Create the enclosing directory if not existent already.\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "    #  Start training.\n",
    "    trainer_lodestar_exp.fit(lodestar_exp, dataloader)\n",
    "    torch.save(lodestar_exp.state_dict(), lodestar_exp_path)\n",
    "else:\n",
    "    #  Load pre-existing weights.\n",
    "    lodestar_exp.load_state_dict(\n",
    "        torch.load(\n",
    "            lodestar_exp_path,\n",
    "            weights_only=True,\n",
    "        )\n",
    "    )\n",
    "    print(\"Loaded preexisting LodeSTAR weights trained on experimental data\")\n",
    "\n",
    "# Switch the model to evaluation mode.\n",
    "lodestar_exp.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13c1ea",
   "metadata": {},
   "source": [
    "### Infer on experimental image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d248163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameters for inference.\n",
    "alpha = 0.05\n",
    "beta = 1 - alpha \n",
    "cutoff = 0.005\n",
    "#  Extract predictions and convert into a tensor.\n",
    "prediction_exp = lodestar_exp(image_of_particles_exp_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction.\n",
    "mass_feature_exp = prediction_exp[2]\n",
    "\n",
    "#  Infer positions from experimental image.\n",
    "detections_lodestar_experimental = lodestar_exp.detect(\n",
    "    image_of_particles_exp_tensor,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    mode=\"constant\",\n",
    "    cutoff=cutoff,\n",
    ")[0]\n",
    "                         \n",
    "print(f\"Found {len(detections_lodestar_experimental[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabc72e",
   "metadata": {},
   "source": [
    "### Display predictions of experimental image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348473d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size.\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(image_of_particles_exp), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Experimental image\")\n",
    "\n",
    "#  Subfigure 2, mass feature distribution.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature_exp, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass features predicted by LodeSTAR\")\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecc660",
   "metadata": {},
   "source": [
    "Overlay the predicted positions on the experimental image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cddcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predicted_positions(\n",
    "   image=image_of_particles_exp[0, 0, :, :],\n",
    "   predicted_positions= detections_lodestar_experimental,\n",
    "   title=\"Method 4: LodeSTAR on experimental image\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_tutorial_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
