{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cee9746",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aarondomenzain/tracking-softmatter-aarond/blob/tracking-softmatter-aarond-dev/tutorial/detection/detection_spheres.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5389-e5c9-4ff1-8ab7-c8c998a829d2",
   "metadata": {},
   "source": [
    "# Particle Tracking Tutorial: Particle Detection and Localization\n",
    "In this tutorial, you’ll explore different methods to detect and localize particles in microscopy images — using both simulated data and real experimental images.\n",
    "\n",
    "You’ll start by generating a simulated image of microscopic particles, mimicking what you might see in a soft matter or biophysics experiment. Since you know the true particle positions in the simulation (i.e., the 'ground truth'), you can directly evaluate how well each detection method performs.\n",
    "\n",
    "Here’s what you’ll test and compare:\n",
    "\n",
    "- Thresholding and Connected Components\n",
    "\n",
    "- Crocker and Grier (using trackpy — a classic in particle tracking)\n",
    "\n",
    "- U-Net (a neural network for image segmentation)\n",
    "\n",
    "- LodeSTAR (an unsupervised deep learning method)\n",
    "\n",
    "You’ll be using Python libraries like NumPy, SciPy, Matplotlib, scikit-image, PyTorch, DeepTrack, and Deeplay. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fb0df",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Import the Required Libraries and Load Utility Functions](#import-the-required-libraries-and-load-utility-functions)\n",
    "1. [Load and Visualize Experimental Images](#load-and-visualize-experimental-images)\n",
    "2. [Simulate Realistic Images with DeepTrack](#simulate-realistic-images-with-deeptrack)\n",
    "    - [Simulate a Single Particle](#simulate-a-single-particle)\n",
    "    - [Simulate a Test Image to Benchmark Detection Methods](#simulate-a-test-image-to-benchmark-detection-methods)\n",
    "3. [Method 1: Thresholding and Connected Components](#3-method-1-thresholding-and-connected-components)\n",
    "    - [Apply Intensity Thresholding](#apply-intensity-thresholding)\n",
    "    - [Determine Centroids with Connected Components Analysis](#determine-centroids-with-connected-components-analysis)\n",
    "    - [Evaluate the Performance of the Method](#evaluate-the-performance-of-the-method)\n",
    "      - [Refine Localization Using Radial Symmetry](#refine-localization-using-radial-symmetry)\n",
    "    - [Apply the Method to Experiments](#apply-the-method-to-experiments)\n",
    "\n",
    "4. [Method 2: Crocker and Grier](#method-2-crocker-and-grier)\n",
    "    - [Estimate Particle Size from Simulations](#estimate-particle-size-from-simulations)\n",
    "    - [Apply the Crocker and Grier Method to Simulations](#apply-the-crocker-and-grier-method-to-simulations)\n",
    "    - [Apply the Crocker and Grier Method to Experiments](#apply-the-crocker-and-grier-method-to-experiments)\n",
    "\n",
    "5. [Method 3: U-Net](#method-3-u-net)\n",
    "    - [Create a Training Dataset](#create-a-training-dataset)\n",
    "    - [Define a U-Net Model with Deeplay](#define-a-u-net-model-with-deeplay)\n",
    "    - [Train the U-Net model](#train-the-u-net-model)\n",
    "    - [Apply the Trained U-Net to the Simulations](#apply-the-trained-u-net-to-simulations)\n",
    "    - [Apply the Trained U-Net to the Experiments](#apply-the-trained-u-net-to-experiments)\n",
    "\n",
    "6. [Method 4: LodeSTAR](#method-4-lodestar)\n",
    "    - [Simulate Training Data for LodeSTAR](#simulate-training-data-for-lodestar)\n",
    "    - [Train LodeSTAR and Apply it to Simulations](#train-lodestar-and-apply-it-to-simulations)\n",
    "    - [Apply the Trained LodeSTAR to Experiments](#apply-the-trained-lodestar-to-experiments)\n",
    "    - [Train LodeSTAR with Experiments](#train-lodestar-with-experimentas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da54cc",
   "metadata": {},
   "source": [
    "## Import the Required Libraries and Load Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817b977",
   "metadata": {},
   "source": [
    "Uncomment the next cell if running on Google Colab/Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c86909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deeptrack deeplay trackpy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab7e2d",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3c704-1473-4998-bf85-e07aa6ff1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries.\n",
    "#import math  # Mathematical operations.\n",
    "import os  # Operating system dependent functionality.\n",
    "import random  # Generate random numbers.\n",
    "import sys  # System-specific parameters and functions.\n",
    "\n",
    "# External libraries.\n",
    "#import cv2  # OpenCV (Computer Vision).\n",
    "import deeptrack as dt  # DeepTrack.\n",
    "import deeplay as dl  # Deeplay.\n",
    "from deeptrack.extras.radialcenter import radialcenter as rc \n",
    "from itertools import cycle # Iterate over a list.\n",
    "import imageio  # Creates images/video from data.\n",
    "import matplotlib # Plotting package\n",
    "import matplotlib.pyplot as plt  # Plotting package with additional funcitons.\n",
    "import numpy as np  # Scientific computing using arrays.\n",
    "import scipy  # Optimized for linear algebra, signal processing.\n",
    "import skimage  # Image analysis (scikit).\n",
    "import torch  # PyTorch library for neural network applications.\n",
    "import trackpy as tp  # Particle tracking package (Crocker & Grier method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d5251",
   "metadata": {},
   "source": [
    "Load a set of custom functions defined specifically for this notebook from the `utils_detection.py` file. For detailed documentation of each function, refer to the comments and docstrings within `utils_detection.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions and utilities for dataset generation and visualization.\n",
    "# Sys append a folder to the path.\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\", \"utils\")))\n",
    "\n",
    "# Import all the functions contained in the file utils/utils_detection.py.\n",
    "import utils_detection as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a1719",
   "metadata": {},
   "source": [
    "Set random seeds to make results reproducible across runs, especially during training and data simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed seed value\n",
    "seed = 76\n",
    "\n",
    "# Python, NumPy, and PyTorch (CPU)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Only set CUDA seeds if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Seeds set to {seed} (with CUDA: {torch.cuda.is_available()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b6d4d-cc42-4522-8299-8b2cc8d73d9f",
   "metadata": {},
   "source": [
    "## Load and Visualize Experimental Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2788ddc",
   "metadata": {},
   "source": [
    "Load a `.tif` image from the `images/` folder and display it using scikit-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4594775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and image file name.\n",
    "image_folder = \"images\"\n",
    "image_file_name = \"JT008_AFM_15.tif\"\n",
    "\n",
    "# Construct the full path.\n",
    "image_path = os.path.join(image_folder, image_file_name)\n",
    "\n",
    "# Create an instance of the loaded file using SciKit.\n",
    "image = skimage.io.imread(image_path)\n",
    "\n",
    "# Crop the image to a square.\n",
    "#image = image[:1024, :1024]\n",
    "image = image[:640, :640]\n",
    "\n",
    "# Print the name of the selected file.\n",
    "print(\"Selected file:\", image_path)\n",
    "\n",
    "# Open a figure instance.\n",
    "plt.figure()\n",
    "\n",
    "# Plot the image in grayscale.\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Experimental Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the image size in pixels.\n",
    "print(f\"Size of imported image in pixels: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c864a5-b676-4333-ba03-30d727494cc4",
   "metadata": {},
   "source": [
    "Manually select and display a single particle by specifying its centroid coordinates (x,y) and a box width.\n",
    "\n",
    "**Note:** Images in NumPy are indexed as `(row, column)`, which corresponds to (y, x) in Cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b4256-0818-44ab-8f66-c3e3f07e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box size to zoom in an individual particle.\n",
    "width = 28\n",
    "\n",
    "# Coordinates of the center of the particle.\n",
    "x_center = 212\n",
    "y_center = 329\n",
    "\n",
    "# Calculate top-left corner of the crop.\n",
    "x = x_center - width // 2\n",
    "y = y_center - width // 2\n",
    "\n",
    "# Select a crop as a subset of the entire image.\n",
    "crop = image[y:y + width, x:x + width]  # row (y), column (x)\n",
    "\n",
    "# Initialize figure instance.\n",
    "fig = plt.figure()\n",
    "\n",
    "# Draw a red rectangle around the crop.\n",
    "fig.add_subplot(111)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Experimental Image\", size=13)\n",
    "plt.plot([x, x, x+width, x+width, x],[y, y+width, y+width, y, y], 'r-')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot the rectangle on the top right corner.\n",
    "fig.add_subplot(555)\n",
    "plt.imshow(crop, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aeb622-c5d3-42f5-8668-419647f7be24",
   "metadata": {},
   "source": [
    "## Simulate Realistic Images with DeepTrack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d116f",
   "metadata": {},
   "source": [
    "DeepTrack allows you to simulate physically realistic microscopy images, enabling precise control over imaging parameters and particle properties. These simulations provide ground-truth data, making them ideal for benchmarking classical and AI-based tracking methods, as well as for training neural networks in a controlled environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23762899",
   "metadata": {},
   "source": [
    "### Simulate a Single Particle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c69915",
   "metadata": {},
   "source": [
    "Adjust the simulation parameters to accurately replicate the features observed in the cropped region of the experimental image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b64cc",
   "metadata": {},
   "source": [
    "**Note:** While the experimental image was acquired using a non-optical technique (Atomic Force Microscopy), DeepTrack is versatile enough to simulate such images as well. However, in this case, imaging parameters have not physical meaning and can have unrealistic values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d7705-e251-4cee-ae05-1710ada00300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the box width.\n",
    "image_size = width  \n",
    "\n",
    "# Size of a pixel in nanometers in the output image.\n",
    "pixel_size_nm = 100 # In nm.\n",
    "\n",
    "# Radius of the particle.\n",
    "particle_radius = 950 # In nm.\n",
    "\n",
    "# Define central spherical scatterer.\n",
    "sphere = dt.Sphere(\n",
    "    position=0.5 * np.array([image_size, image_size]),\n",
    "    z= 0 * dt.units.nm, # Particle in focus.\n",
    "    radius= particle_radius * dt.units.nm,  # Radius in nanometers.\n",
    "    intensity= 5E4,  # Field magnitude squared.\n",
    ")\n",
    "\n",
    "# Simulate the properties of the fluorescence microscope.\n",
    "optics = dt.Fluorescence(\n",
    "    NA=0.5,  # Numerical aperture.\n",
    "    wavelength=370 * dt.units.nm,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
    ")\n",
    "\n",
    "# Apply transformations.\n",
    "sim_crop = optics(sphere) >> dt.Background(3800)\n",
    "\n",
    "# Turn the crop into a NumPy array.\n",
    "simulated_crop = np.squeeze(sim_crop())\n",
    "\n",
    "# Plot the simulated and experimental crops.\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Simulated crop.\n",
    "plot = axes[0].imshow(simulated_crop, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Simulated Crop\")\n",
    "\n",
    "# Experimental crop.\n",
    "axes[1].imshow(crop, cmap=\"gray\")  \n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Experimental Crop\")\n",
    "\n",
    "# Adjust layout and show plot.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79257d81",
   "metadata": {},
   "source": [
    "Extract and visualize the raw intensity profiles along the central horizontal line of both the simulated and experimental crops. This comparison helps evaluate how well the simulation reproduces the intensity distribution observed in real microscopy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdafb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the raw intensity profile of the simulated experimental crops.\n",
    "simulated_crop_intensity = simulated_crop[image_size // 2, :]\n",
    "experimental_crop_intensity = crop[image_size // 2, :]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(simulated_crop_intensity, label=\"Simulated Crop\", color=\"orange\")\n",
    "ax.plot(experimental_crop_intensity, label=\"Experimental Crop\", color=\"blue\")\n",
    "ax.set_xlabel(\"Pixel\")\n",
    "ax.set_ylabel(\"Intensity\")\n",
    "ax.set_title(\"Intensity Profile Comparison\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b890387-3fd7-4e6c-9651-76d067b9efae",
   "metadata": {},
   "source": [
    "### Simulate a Test Image to Benchmark Detection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc6fda",
   "metadata": {},
   "source": [
    "Create a simulated image containing non-overlapping spherical particles. Begin by generating their coordinates—these will serve as the ground-truth positions. Then, use DeepTrack to render optically realistic particles at these coordinates, resulting in a physically plausible microscopy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d981d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the simulation.\n",
    "image_size = 256\n",
    "N_particles = 128\n",
    "particle_radius = 900  # Particle radius in nm.\n",
    "\n",
    "# Dictionary for particle properties. Dimensions are set with lambda\n",
    "# functions to introduce variety to the dataset.\n",
    "sphere_properties = {\n",
    "    \"radius\": lambda: np.random.uniform(0.85, 1.05) * particle_radius * dt.units.nm,\n",
    "    \"intensity\": lambda: np.random.uniform(0.7, 1.3) * 5E4,\n",
    "}\n",
    "\n",
    "# Set the optical properties of the microscope.  \n",
    "optics_properties = dt.Fluorescence(\n",
    "    NA=0.5,  # Numerical aperture.\n",
    "    wavelength=370 * dt.units.nm,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
    ")\n",
    "\n",
    "# Generate ground truth positions.\n",
    "gt_pos = utils.generate_centroids(\n",
    "    num_particles=N_particles,\n",
    "    image_size=image_size,\n",
    "    particle_radius=particle_radius, \n",
    ")\n",
    "\n",
    "# Simulate image.\n",
    "simulated_image = utils.transform_to_video(\n",
    "    gt_pos,\n",
    "    image_size=image_size,\n",
    "    core_particle_props=sphere_properties,\n",
    "    optics_props=optics_properties,\n",
    ")\n",
    "\n",
    "# Generate the ground truth as Gaussian clouds centered at the position of the\n",
    "# spheres. The standard deviation of the Gaussians are set to be proportional \n",
    "# to the radii of the corresponding spherical particles.\n",
    "ground_truth_cloud_size = particle_radius / 3\n",
    "\n",
    "# Generate ground truth map using the positions, radii and intensities of the \n",
    "# particles.\n",
    "simulated_map = utils.create_ground_truth_map(\n",
    "    gt_pos, \n",
    "    image_size=image_size, \n",
    "    sigma=ground_truth_cloud_size, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b7299",
   "metadata": {},
   "source": [
    "Visualize the simulated image and the corresponding ground truth map. Each particle has an associated ground truth cloud with a Gaussian profile, whose standard deviation is proportional to the radius of the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subfigure instance to plot the image and the probability map.\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# Instance for the simulated experimental image. \n",
    "img1 = ax[0].imshow(simulated_image, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Simulated Image\")  \n",
    "\n",
    "# Instance for the probability map.\n",
    "img2 = ax[1].imshow(simulated_map, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Simulated Probability Map (Gaussian)\") \n",
    "\n",
    "# Adjust layout to avoid overlap.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72d222-07b8-4001-b5ac-ea169cfb4785",
   "metadata": {},
   "source": [
    "## Method 1: Thresholding and Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5720e5",
   "metadata": {},
   "source": [
    "In a grayscale image, each pixel has an intensity value, where higher values typically correspond to stronger signals—often indicating the presence of particles—while lower values represent the background.\n",
    "\n",
    "A simple and effective method for particle detection is **intensity thresholding**, which converts the grayscale image into a binary mask. Pixels with intensities above a chosen threshold are marked as foreground (set to 1), and the rest as background (set to 0).\n",
    "\n",
    "If particles appear darker than the background—as in absorption-based or negative stain imaging—the threshold should instead be applied to the lower end of the intensity distribution.\n",
    "\n",
    "After thresholding, **connected component analysis** can be used to label and isolate individual foreground regions. Each connected component represents a candidate particle, enabling basic localization by computing properties such as the centroid (position), area (size), or bounding box (extent) of each region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6564822",
   "metadata": {},
   "source": [
    "### Apply Intensity Thresholding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ce5e1",
   "metadata": {},
   "source": [
    "A robust way to choose the threshold is to use the **intensity quantile**. For example, setting the threshold at the 90th percentile selects the brightest 10% of pixels, which often correspond to particles. This method is more reliable than using the image mean and standard deviation, which can be biased by background-dominated images.\n",
    "\n",
    "Before thresholding, it’s a good idea to smooth the image with a Gaussian filter. This helps suppress noise and small fluctuations that aren’t real particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a34bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Gaussian filter to the image to smooth out noise.\n",
    "filtered_image_sim = scipy.ndimage.gaussian_filter(simulated_image, sigma=1.0)\n",
    "\n",
    "# Flatten the filtered image for thresholding.\n",
    "flattened_image_filtered = filtered_image_sim.flatten()\n",
    "\n",
    "# Define the quantile level (e.g., 90% selects the top 10% brightest pixels).\n",
    "quantile_level = 0.85\n",
    "\n",
    "# Compute the threshold as the intensity value at the given quantile.\n",
    "intensity_threshold = np.quantile(flattened_image_filtered, quantile_level)\n",
    "\n",
    "print(f\"Intensity threshold (quantile={quantile_level*100:.0f}%): {intensity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd353e33",
   "metadata": {},
   "source": [
    "Plot the intensity histogram of the simulated image before and after Gaussian filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the raw image for thresholding.\n",
    "flattened_image_simulated = simulated_image.flatten()\n",
    "\n",
    "# Compute statistics\n",
    "median_intensity_simulated = np.median(flattened_image_simulated)\n",
    "quantile_threshold_simulated = np.quantile(flattened_image_simulated, quantile_level)\n",
    "\n",
    "median_intensity_filtered = np.median(flattened_image_filtered)\n",
    "quantile_threshold_filtered = np.quantile(flattened_image_filtered, quantile_level)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# Histogram for simulated image\n",
    "axes[0].hist(flattened_image_simulated, bins=256, range=(0, 1), color='gray', alpha=0.75, density=True)\n",
    "axes[0].axvline(median_intensity_simulated, color='red', linestyle='dashed', linewidth=1.5, label=\"Median\")\n",
    "axes[0].axvline(quantile_threshold_simulated, color='green', linestyle='dashed', linewidth=1.5, label=f\"{int(quantile_level*100)}th Percentile\")\n",
    "axes[0].set_title(\"Simulated Image\", fontsize=14)\n",
    "axes[0].set_yticklabels([])  # Remove y labels\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Histogram for filtered image\n",
    "axes[1].hist(flattened_image_filtered, bins=256, range=(0, 1), color='gray', alpha=0.75)\n",
    "axes[1].axvline(median_intensity_filtered, color='red', linestyle='dashed', linewidth=1.5, label=\"Median\")\n",
    "axes[1].axvline(quantile_threshold_filtered, color='green', linestyle='dashed', linewidth=1.5, label=f\"{int(quantile_level*100)}th Percentile\")\n",
    "axes[1].set_title(\"Simulated Image + Gaussian Filter\", fontsize=14)\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Set common x and y labels\n",
    "fig.supxlabel(\"Normalized Intensity\", fontsize=12)\n",
    "fig.supylabel(\"Normalized Frequency [counts]\", fontsize=12)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f267b",
   "metadata": {},
   "source": [
    "Apply the threshold to generate a binary mask, then display it together with the original image for visual validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d1cbc-2e83-4087-9e45-63bdf1aab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the binary mask by applying the threshold.\n",
    "simulated_mask = filtered_image_sim > intensity_threshold\n",
    "\n",
    "# Ensure the mask is a 2D array.\n",
    "simulated_mask = np.squeeze(simulated_mask)\n",
    "\n",
    "# Plot the image with the calculated mask.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=simulated_image, \n",
    "    mask=simulated_mask, \n",
    "    title=\"Mask with Intensity Thresholding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48868ccd-04bd-4699-9bca-c682dbcf8653",
   "metadata": {},
   "source": [
    "### Determine Centroids with Connected Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba790035",
   "metadata": {},
   "source": [
    "Detect objects from the binary mask and extract their centroids using the scikit-image library. This is typically done by identifying connected components and computing their center of mass, which provides an estimate of each particle’s position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de013b1-04bc-4361-a73a-3ae0b4d20fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to extract positions from mask.\n",
    "detections_M1 = utils.mask_to_positions(simulated_mask)\n",
    "print(f\"Found {len(detections_M1[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8322201",
   "metadata": {},
   "source": [
    "Plot the extracted centroids on top of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1224ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plotting function to compare the detection with the ground truth.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image, \n",
    "    predicted_positions=detections_M1, \n",
    "    ground_truth_positions=gt_pos, \n",
    "    title=\"Method 1 - Simulated Image\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6739-64ed-4946-bbe4-f7ed428ad22f",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ad3ff",
   "metadata": {},
   "source": [
    "Apply the `evaluate_locs` function that calculates evaluation metrics and print them. It calculates True Positives (TP), False Positives (FP), False Negatives (FN), F1 score (F1) and Mean Square Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e920e9-2229-44db-aa77-fcc393d31dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics can be retrieved if returned outputs are asigned to a variable:\n",
    "# TP, FP, FN, F1, RMSE = utils.evaluate_locs(detections_M1, gt_pos, distance_th=particle_radius).\n",
    "\n",
    "utils.evaluate_locs(detections_M1, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9781519-d652-4bac-b831-026038460c42",
   "metadata": {},
   "source": [
    "#### Refine Localization Using Radial Symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c7e4a",
   "metadata": {},
   "source": [
    "The utility function `locate_particle_centers` refines particle positions by leveraging the radial symmetry around each initially detected center. It focuses on regions of interest surrounding these centers and improves localization accuracy—especially for spherical particles, where the symmetry assumption holds. This approach helps achieve subpixel precision in position estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0636df",
   "metadata": {},
   "source": [
    "Use radial symmetry refinement on top of the method to achieve more accurate centroid positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the location of the centroid of a masked particle optical signal.\n",
    "detections_M1_ref = utils.locate_particle_centers(\n",
    "    detections_M1, \n",
    "    simulated_image, \n",
    "    estimated_radius=particle_radius,\n",
    ")\n",
    "\n",
    "# Use the plotting function to compare the detection with the ground truth.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image, \n",
    "    predicted_positions=detections_M1_ref, \n",
    "    ground_truth_positions=gt_pos, \n",
    "    title=\"Method 1 with Radial Symmetry Refinement - Simulated Image\",\n",
    ")\n",
    "\n",
    "# Evaluate the performance.\n",
    "utils.evaluate_locs(detections_M1_ref, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b575722-b518-4d85-871a-06cb635b9e8e",
   "metadata": {},
   "source": [
    "### Apply the Method to Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ee052",
   "metadata": {},
   "source": [
    "Apply the same method to the experimental image:\n",
    "\n",
    "- Choose a threshold at the 85th percentile, so only the top 15% brightest pixels are kept.\n",
    "- Create a binary mask where pixels above the threshold are marked as potential particles.\n",
    "- Extract initial positions from the mask.\n",
    "- Plot both the predicted positions and the mask for visual inspection.\n",
    "- Print the number of detections found.\n",
    "\n",
    "This gives us a first estimate of particle positions using just thresholding—no machine learning needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a359c4-acba-4b2e-bf05-cd8d8a9184f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85% quantile level for thresholding.\n",
    "quantile_level = 0.85\n",
    "\n",
    "# Intensity threshold for the experimental image.\n",
    "quantile_threshold_exp = np.quantile(image, quantile_level)\n",
    "\n",
    "# Create the binary mask by applying the threshold.\n",
    "mask_exp = image > quantile_threshold_exp\n",
    "\n",
    "# Get predicted positions from the mask.\n",
    "detections_M1_exp = utils.mask_to_positions(mask_exp)\n",
    "\n",
    "# Plot the experimental image with the predicted positions.\n",
    "utils.plot_predicted_positions(\n",
    "    image=image,\n",
    "    predicted_positions=detections_M1_exp,\n",
    "    title=\"Method 1 - Experimental Image\",\n",
    ")\n",
    "\n",
    "# Plot the experimental image and the mask.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=image, \n",
    "    mask=mask_exp, \n",
    "    title=\"Method 1 - Experimental Image\",\n",
    ")\n",
    "\n",
    "# Print the number of detections.\n",
    "print(f\"Found {len(detections_M1_exp[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bfb12-bce1-4be6-894f-b98482c83779",
   "metadata": {},
   "source": [
    "## Method 2: Crocker and Grier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3725b00",
   "metadata": {},
   "source": [
    "A refined approach for particle detection and localization is the Crocker and Grier method. This method identifies local intensity maxima and refines their positions by fitting the surrounding region to a Gaussian-like profile. It offers subpixel accuracy and performs well even in moderately noisy conditions or crowded images.\n",
    "\n",
    "Trackpy provides a built-in implementation of the Crocker and Grier particle detection method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3734b",
   "metadata": {},
   "source": [
    "### Estimate Particle Size from Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51752c",
   "metadata": {},
   "source": [
    "In the trackpy implementation of the Crocker and Grier method, the primary parameter you need to set is the estimated particle size, which must be an odd number.\n",
    "\n",
    "To estimate this value, inspect the simulated image interactively: click on both edges of a representative particle to measure its approximate width. Keep in mind that particle sizes may vary across the image, so this should be treated as an initial guess. You can refine the diameter later by evaluating the quality of the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44984328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib interactive backend .\n",
    "%matplotlib widget\n",
    "utils.interactive_ruler(simulated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb6910",
   "metadata": {},
   "source": [
    "### Apply the Crocker and Grier Method to Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe99192",
   "metadata": {},
   "source": [
    "Use `trackpy.locate` to detect particles in the image. This function returns a DataFrame containing the coordinates of the detected particles along with various feature properties.\n",
    "\n",
    "Extract the (x, y) positions from the DataFrame and compare them to the ground truth coordinates to assess the accuracy of the detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and tune the diameter of particles in pixels, ensuring it is an\n",
    "# odd number. Adjust if necessary.\n",
    "estimated_size = 15\n",
    "\n",
    "# Use the locate built-in function from TrackPy.\n",
    "# Output is type DataFrame([x, y, mass, size, ecc, signal, raw_mass]).\n",
    "localizations_dataframe = tp.locate(\n",
    "    simulated_image, \n",
    "    diameter=estimated_size,\n",
    "    noise_size=4, # Setting for denoising image.\n",
    ")\n",
    "\n",
    "# Extract the predicted positions from DataFrame[\"x\",\"y\"] into a NumPy array.\n",
    "detections_M2 = np.array(localizations_dataframe)[:,:2]\n",
    "\n",
    "# Plot the prediction and the ground truth.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_M2,\n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Method 2 - Simulated Image\",\n",
    ")\n",
    "\n",
    "# Evaluate the performance of tp.locate.\n",
    "utils.evaluate_locs(\n",
    "    detections_M2,\n",
    "    gt_pos,\n",
    "    distance_th=particle_radius,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5797d8-2786-460a-86c9-de8ab51645b9",
   "metadata": {},
   "source": [
    "### Apply the Crocker and Grier Method to Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fcf1f",
   "metadata": {},
   "source": [
    "Interactively estimate particle size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3838216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib interactive backend .\n",
    "%matplotlib widget\n",
    "utils.interactive_ruler(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e066a2",
   "metadata": {},
   "source": [
    "Detect particles in the image and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d9ba9-c1f0-424c-838c-ae391acb2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the estimated diameter if necessary. \n",
    "estimated_size = 15\n",
    "\n",
    "# Use Trackpy.locate to experimental data file 'image'.\n",
    "localizations_dataframe_exp = tp.locate(\n",
    "    image, \n",
    "    diameter=estimated_size,\n",
    "    noise_size=4, # Setting for denoising image.\n",
    "\n",
    ")\n",
    "\n",
    "# Extract the predicted positions from DataFrame[\"x\",\"y\"] into a NumPy array.\n",
    "detections_M2_exp = np.array(localizations_dataframe_exp)[:,:2]\n",
    "\n",
    "# Plot the experimental image with predicted positions overlaid.\n",
    "utils.plot_predicted_positions(\n",
    "    image=image,\n",
    "    predicted_positions=detections_M2_exp,\n",
    "    title=\"Method 2 - Experimental Image\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(detections_M2_exp[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6cb5f-f4d9-45f8-b051-92f8ec153ccb",
   "metadata": {},
   "source": [
    "## Method 3: U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7af2a3",
   "metadata": {},
   "source": [
    "U-Net is a convolutional neural network (CNN) widely used for image segmentation tasks. It performs pixel-wise classification, enabling precise separation between background and objects of interest—ranging from simple particles to complex cells or anatomical structures, depending on the training data.\n",
    "\n",
    "In the context of particle detection, U-Net can be trained to map input microscopy images directly into **probability distributions** that represent the likelihood of a particle being present at each pixel. Instead of assigning a hard classification (particle or background), the network outputs a **dense probability map**—a soft, continuous prediction indicating how likely each pixel is to belong to a particle. This approach is particularly useful in challenging scenarios, such as overlapping particles—where discrete detection may fail—or small, faint particles that traditional methods might miss.\n",
    "\n",
    "Once trained, U-Net generalizes to new images, producing smooth probability maps that can be thresholded to create binary masks for downstream connected-component analysis. For localization, these maps enable the calculation of **intensity-weighted centroids**, allowing subpixel resolution of particle positions and improving localization accuracy. Alternatively, the maps can be analyzed using peak detection algorithms or interpreted as confidence scores to filter or rank detections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75488fa-f1a5-4252-9f06-5df7609af1e7",
   "metadata": {},
   "source": [
    "### Create a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90b159",
   "metadata": {},
   "source": [
    "U-Net is trained using pairs of input images and corresponding target maps, which represent the expected output distribution. These target maps typically consist of Gaussian blobs centered at the true particle locations, encoding both position and uncertainty.\n",
    "\n",
    "To generate training data, you can simulate particle images using DeepTrack, along with their associated probability maps to serve as ground truth.\n",
    "\n",
    "If a dataset has already been generated and saved, you can simply load it and skip the simulation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762c048-9ed8-45c8-bd3b-758c21329245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples, image size, and particles.\n",
    "num_samples = 128\n",
    "image_size = 64\n",
    "max_num_particles = 10\n",
    "force_simulation = True  # Flag to force simulation even if data exists.\n",
    "\n",
    "# Simulate the properties of the fluorescence microscope.\n",
    "optics_properties = dt.Fluorescence(\n",
    "    NA=0.5,  # Numerical aperture.\n",
    "    wavelength=370 * dt.units.nm,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
    ")\n",
    "\n",
    "# Create path to store training dataset.\n",
    "folder_name = \"UNet\"\n",
    "training_dataset_filename = \"UNet_training_dataset_spheres.npz\"\n",
    "training_dataset_folder = os.path.join(folder_name, \"training_data\")\n",
    "training_dataset_filepath = os.path.join(\n",
    "    training_dataset_folder, \n",
    "    training_dataset_filename,\n",
    ")\n",
    "\n",
    "# Create the enclosing directory if not existent already.\n",
    "if not os.path.exists(training_dataset_folder):\n",
    "    os.makedirs(training_dataset_folder, exist_ok=True)\n",
    "\n",
    "# Try to load preexisting data, if not available or forced, raise the\n",
    "# exception error to generate new data.\n",
    "try:\n",
    "    if force_simulation: \n",
    "        # Raise the exception error if simulation is forced.\n",
    "        raise FileNotFoundError(\"Forced simulation by user request.\")\n",
    "    \n",
    "    if not os.path.isfile(training_dataset_filepath):\n",
    "        # If file is not found, start training.    \n",
    "        raise FileNotFoundError(\n",
    "            \"Training dataset file not found. Starting simulation.\"\n",
    "        )\n",
    "    \n",
    "    # Load existing data\n",
    "    data = np.load(training_dataset_filepath)\n",
    "    images = data['images']\n",
    "    maps = data['maps']\n",
    "    Nsamples = len(images)\n",
    "    print(f\"Loaded file: {training_dataset_filepath}\")\n",
    "        \n",
    "# Handle the case of either file not found or forced training.\n",
    "except FileNotFoundError:\n",
    "    # Generate new dataset if file not found or simulation is forced\n",
    "    images, maps = utils.generate_particle_dataset(\n",
    "        num_samples,\n",
    "        image_size,\n",
    "        max_num_particles,\n",
    "        sphere_properties,\n",
    "        optics_properties=optics_properties,\n",
    "    )\n",
    "    \n",
    "    # Save the simulated training dataset.\n",
    "    np.savez(training_dataset_filepath, images=images, maps=maps)\n",
    "    print(f\"Training dataset saved in: {training_dataset_filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453aaeb",
   "metadata": {},
   "source": [
    "Before training the U-Net model with Deeplay, ensure that the input dataset has the shape (N, X, Y, C), where:\n",
    "\n",
    "N: Number of samples (simulated images)\\\n",
    "X: Width of the image (pixels along the horizontal axis)\\\n",
    "Y: Height of the image (pixels along the vertical axis)\\\n",
    "C: Number of channels (use 1 for grayscale images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2234b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image dimensions: {images.shape}\")\n",
    "print(f\"Maps dimensions: {maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fd2d5",
   "metadata": {},
   "source": [
    "Visualize the simulated training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image and its corresponding probability maps and mask to show.\n",
    "selected_image_index = np.random.randint(0, len(images))\n",
    "\n",
    "# Extract the image and probability map from 4D arrays.\n",
    "selected_image = np.squeeze(images[selected_image_index])\n",
    "simulated_probability_map = np.squeeze(maps[selected_image_index])\n",
    "\n",
    "# Plot the image as the first subplot.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=selected_image,\n",
    "    ground_truth_map=simulated_probability_map,\n",
    "    title=f\"Training dataset element: {selected_image_index + 1}/{len(images)}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3bb9d-a80a-478a-8566-5bbab51333b3",
   "metadata": {},
   "source": [
    "### Define a U-Net model using Deeplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b80a44",
   "metadata": {},
   "source": [
    "The name _U-Net_ comes from its characteristic U-shaped architecture, which features a symmetric encoder–decoder structure. The encoder (downsampling path) captures contextual information by progressively reducing spatial resolution, while the decoder (upsampling path) restores spatial detail. Skip connections between corresponding levels of the encoder and decoder help retain fine-grained features, improving segmentation accuracy—especially for small or faint objects.\n",
    "\n",
    "You'll define a 2D U-Net where both the input and output images have a single channel, corresponding to grayscale intensity. The goal is for the model to learn a mapping from the experimental image to a spatial probability density map that indicates the likelihood of particle presence at each pixel.\n",
    "\n",
    "The U-Net should be deep enough to extract meaningful features from the input. As a rule of thumb, with a kernel size of 3×3, a network with N convolutional layers has a receptive field of approximately (3×3)^N. For example, using three layers yields a receptive field of 27×27 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb159ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output channels are set to 1, since both images and probability\n",
    "# maps only have one color channel.\n",
    "unet = dl.UNet2d(\n",
    "    in_channels=1, \n",
    "    channels=[16, 32, 64], \n",
    "    out_channels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e56ba9",
   "metadata": {},
   "source": [
    "Compile the model for the task of regressing the masks from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c64c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A U-Net regressor is defined using a deeplay model. The loss function is Mean\n",
    "# Squared Error Loss, since the output is a continuous intensity map.\n",
    "unet_reg = dl.Regressor(\n",
    "    model=unet, \n",
    "    loss=torch.nn.MSELoss(), \n",
    "    optimizer=dl.Adam(lr=1e-3),\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5d9e0",
   "metadata": {},
   "source": [
    "Define the training pipeline using Deeptrack to generate the training dataset\n",
    "The training pipeline is composed of the simulated images and the probability maps, together with instructions of value normalization and a selector of images, all of which are instances of Deeplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image selector with a random picker. This is performed in order to properly \n",
    "# link an element in maps array with its corresponding element in images \n",
    "# array.\n",
    "selector = dt.Lambda(\n",
    "    lambda i: lambda x: x[i], i=lambda l: np.random.randint(l), l=len(images)\n",
    ")\n",
    "\n",
    "# Apply augmentations of added Gaussian noise only to images.\n",
    "images_augmentations = (\n",
    "    dt.Value(images)\n",
    "    >> dt.Gaussian(0.0, 0.015)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), \n",
    "        lambda: np.random.uniform(0.9, 1.0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Apply normalization to ground truth map.\n",
    "maps = dt.Value(maps) >> dt.NormalizeMinMax(0.0, 1.0)\n",
    "\n",
    "# Training pipeline consists of two paired lists: images and maps. \n",
    "# Images are augmented and normalized, while maps are only normalized.\n",
    "# The selector randomly selects an image and its corresponding ground truth map.\n",
    "# The pipeline is then passed to the deeplay model.\n",
    "pipeline = (images_augmentations & maps) >> selector\n",
    "\n",
    "# Create the training dataset instance in PyTorch\n",
    "train_dataset = dt.pytorch.Dataset(pipeline, length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56847fc3",
   "metadata": {},
   "source": [
    "Randomly select and display one augmented crop from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe52aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "sanity_check_pipeline_augmentation = np.squeeze(pipeline.update().resolve())\n",
    "sanity_check_image_augmentation = sanity_check_pipeline_augmentation[0]\n",
    "sanity_check_map_augmentation = sanity_check_pipeline_augmentation[1]\n",
    "\n",
    "# # Plot the image as the first subplot\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    image=sanity_check_image_augmentation,\n",
    "    ground_truth_map=sanity_check_map_augmentation,\n",
    "    title=f\"Random Augmentation from Training Pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797d694-9bab-4505-93ab-4d36d4174f30",
   "metadata": {},
   "source": [
    "### Train the U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d41712",
   "metadata": {},
   "source": [
    "Create a dataloader to iterate over the training dataset in batches ensuring randomized sampling at each epoch. Initialize a Trainer from Deeplay to train the U-Net model for up to 64 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638011-81a0-482b-a7eb-4aeb776312d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for training.\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize UNet trainer with automatic accelerator (e.g., CUDA if available).\n",
    "unet_trainer = dl.Trainer(max_epochs=64, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34721c",
   "metadata": {},
   "source": [
    "Attempt to load pre-trained weights for the U-Net model. If the weights file does not exist, train the model from scratch and save the weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force training if desired.\n",
    "force_training = True\n",
    "\n",
    "# Define the file paths for the model weights.\n",
    "unet_path = \"UNet_model_spheres.pth\"\n",
    "unet_reg_path = \"UNet_reg_spheres.pth\"\n",
    "\n",
    "# Define folder and construct full file paths.\n",
    "folder_name = \"UNet\"\n",
    "unet_path = os.path.join(folder_name, unet_path)\n",
    "unet_reg_path = os.path.join(folder_name, unet_reg_path)\n",
    "\n",
    "# Load preexisting weights if they exist and training is not forced.\n",
    "if not force_training and os.path.exists(unet_path) and os.path.exists(unet_reg_path):\n",
    "    unet.load_state_dict(torch.load(unet_path, weights_only=True))\n",
    "    unet_reg.load_state_dict(torch.load(unet_reg_path, weights_only=True))\n",
    "    print(f\"Loaded preexisting U-Net weights from '{folder_name}/'.\")\n",
    "else:\n",
    "    print(\"Training U-Net model (either forced or weights not found).\")\n",
    "    \n",
    "    # Ensure the save directory exists.\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Train the U-Net model.\n",
    "    unet_trainer.fit(unet_reg, train_loader)\n",
    "\n",
    "    # Save the weights.\n",
    "    torch.save(unet.state_dict(), unet_path)\n",
    "    torch.save(unet_reg.state_dict(), unet_reg_path)\n",
    "    print(f\"Saved trained U-Net weights to '{folder_name}/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823a1c0",
   "metadata": {},
   "source": [
    "### Apply the Trained U-Net to the Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd48b2e",
   "metadata": {},
   "source": [
    "Select a new image that was not used during training and use the trained U-Net to infer the probability map, leveraging the knowledge the model has learned. This output represents the predicted likelihood of particle presence at each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a79a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image to apply the learning by the U-Net.\n",
    "image_of_particles = np.array(simulated_image).astype(np.float32)\n",
    "print(f\" Shape of selected image: {image_of_particles.shape} \")\n",
    "\n",
    "# Normalize intensity of image to (0,1).\n",
    "image_of_particles = utils.normalize_min_max(image_of_particles)\n",
    "\n",
    "# Create a new axis and rearrange to (N,X,Y,C) format.\n",
    "# In this case, it should be (1, 1, 256, 256).\n",
    "image_of_particles = np.expand_dims(image_of_particles, axis=0)\n",
    "image_of_particles = image_of_particles.transpose(0, 3, 1, 2)\n",
    "print(f\" Shape of selected image after transposing to match the (N, C, X, Y): {image_of_particles.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f834b6d",
   "metadata": {},
   "source": [
    "Convert the selected image to a PyTorch tensor and pass it through the trained U-Net to infer the probability map of particle locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to analyze into a PyTorch tensor.\n",
    "image_of_particles_tensor = torch.from_numpy(image_of_particles)\n",
    "\n",
    "# Apply the UNet to the loaded image.\n",
    "pred_maps_tensor = unet_reg(image_of_particles_tensor)\n",
    "\n",
    "# Convert to NumPy array.\n",
    "pred_maps_array = pred_maps_tensor[0,0,:,:].detach().numpy()\n",
    "\n",
    "# Normalize intensity.\n",
    "pred_maps_array = utils.normalize_min_max(pred_maps_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2281d2",
   "metadata": {},
   "source": [
    "Apply thresholding to the predicted probability map to isolate potential particle regions.\n",
    "\n",
    "Calculate the centroids of the detected regions using the probability map as a weight. This enables subpixel localization by emphasizing the most confident regions of each detection. \n",
    "\n",
    "Evaluate performance by comparing the predicted coordinates with ground-truth positions.\n",
    "\n",
    "Visualize the results by displaying the original image with the localized particle positions overlaid, allowing you to qualitatively assess the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87363d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply thresholding to the predicted ground truth map.\n",
    "pred_mask_unet = pred_maps_array > 0.25\n",
    "\n",
    "# Convert the masked ground truth map to positions.\n",
    "detections_unet = utils.mask_to_positions(pred_mask_unet, pred_maps_array)\n",
    "\n",
    "# Show the predicted posiions overlaid with the ground truth on the simulated\n",
    "# image.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_unet, \n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Method 3 - Simulated Image\",\n",
    ")\n",
    "# Print the number of detections.\n",
    "print(f\"Found {len(detections_unet[:,1])} detections.\")\n",
    "\n",
    "# Measure the performance.\n",
    "utils.evaluate_locs(detections_unet, gt_pos, distance_th=particle_radius);\n",
    "\n",
    "# Plot the predicted ground truth map and its masked version.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    mask=pred_mask_unet,\n",
    "    ground_truth_map=pred_maps_array,\n",
    "    title=\"Method 3 - Simulated Image\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2d1d0",
   "metadata": {},
   "source": [
    "### Apply the Trained U-Net to the Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1c2e5",
   "metadata": {},
   "source": [
    "Use the trained U-Net model to predict where particles are in the experimental image:\n",
    "\n",
    "- Normalize the image so pixel values are between 0 and 1.\n",
    "- Reshape the image to match the input format expected by the network: (N, C, X, Y) → where N=1 (batch size), and C=1 (grayscale).\n",
    "- Convert the image to a PyTorch tensor and feed it to the U-Net.\n",
    "- The model returns a probability map showing where it thinks particles are most likely to be.\n",
    "- Normalize the predicted map again to scale between 0 and 1.\n",
    "- Apply a threshold (e.g., > 0.2) to the probability map to create a binary mask.\n",
    "- Extract particle positions from this mask.\n",
    "- Plot both the experimental image with predicted positions and the model's output map + masked version for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d468f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize intensity of image to (0,1).\n",
    "image_of_particles_exp = utils.normalize_min_max(image)\n",
    "\n",
    "# Reshape the array to match the (N, C, X, Y) format.\n",
    "image_of_particles_exp = image_of_particles_exp[np.newaxis, np.newaxis, ...]\n",
    "\n",
    "# Ensure that the image is a numpy array.\n",
    "image_of_particles_exp = np.array(image_of_particles_exp).astype(np.float32) \n",
    "\n",
    "# Convert the image to analyze into a PyTorch tensor.\n",
    "image_of_particles_exp_tensor = torch.from_numpy(image_of_particles_exp)\n",
    "\n",
    "# Apply the UNet to the loaded image.\n",
    "pred_maps_unet_exp = unet_reg(image_of_particles_exp_tensor)\n",
    "\n",
    "# Convert the prediction to a NumPy array for easy plotting.\n",
    "pred_maps_unet_exp_array = pred_maps_unet_exp[0, 0, :, :].detach().numpy()\n",
    "\n",
    "# Normalize intensity to (0,1).\n",
    "pred_maps_unet_exp_array = utils.normalize_min_max(pred_maps_unet_exp_array)\n",
    "\n",
    "# Calculate a mask from the ground truth map with thresholding method.\n",
    "pred_mask_unet_exp = pred_maps_unet_exp_array > 0.2\n",
    "\n",
    "# Extract the positions from the mask.\n",
    "detections_unet_exp = utils.mask_to_positions(pred_mask_unet_exp)\n",
    "\n",
    "# Plot the experimental image with the positions predicted by U-Net.\n",
    "utils.plot_predicted_positions(\n",
    "    image=image, \n",
    "    predicted_positions=detections_unet_exp, \n",
    "    title=\"Method 3 - Experimental Image\",\n",
    ")\n",
    "\n",
    "# Plot the predicted ground truth map and its masked version.\n",
    "utils.plot_image_mask_ground_truth_map(\n",
    "    mask=pred_mask_unet_exp,\n",
    "    ground_truth_map=pred_maps_unet_exp_array,\n",
    "    title=\"Method 3 - Experimental Image\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e845e",
   "metadata": {},
   "source": [
    "## Method 4: LodeSTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dccf5-8255-4032-bbe0-4a3ae9685d06",
   "metadata": {},
   "source": [
    "LodeSTAR is a self-supervised method for object detection that doesn’t require labeled data. You can even train it using a single input image—no manual annotations needed!\n",
    "\n",
    "The idea is simple but powerful: LodeSTAR learns to detect particles by finding consistent features across randomly transformed versions of the same image. These transformations can include translation, rotation, or scaling.\n",
    "\n",
    "By learning to predict the coordinate shifts between the transformed images, the network becomes very good at localizing objects—even under noisy or experimental conditions.\n",
    "\n",
    "This makes LodeSTAR especially useful when ground-truth labels are unavailable or unreliable, which is often the case in real microscopy work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70f00e",
   "metadata": {},
   "source": [
    "### Simulate Training Data for LodeSTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5376714",
   "metadata": {},
   "source": [
    "To mimic real experimental conditions, we use DeepTrack to generate a set of synthetic images, each containing a single particle. These are produced using a mix of simulation and data augmentation. The particle’s size and intensity are randomized within realistic bounds, making the dataset more diverse and representative of what you'd expect in actual microscopy data.\n",
    "\n",
    "This setup reflects real use cases, where LodeSTAR is trained on augmentations of a single experimental crop.\n",
    "\n",
    "If the dataset already exists, you can simply load it; otherwise, new crops are simulated and saved.\n",
    "\n",
    "Finally, a few sample crops are visualized to confirm they look physically realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples, image size, and particles.\n",
    "num_samples = 128\n",
    "image_size = 30\n",
    "max_num_particles = 1\n",
    "force_simulation = True  # Flag to force simulation even if data exists.\n",
    "\n",
    "# Create path to store training dataset.\n",
    "folder_name = \"LodeSTAR\"\n",
    "training_dataset_filename = \"LodeSTAR_training_dataset_spheres.npz\"\n",
    "training_dataset_folder = os.path.join(folder_name, \"training_data\")\n",
    "training_dataset_filepath = os.path.join(\n",
    "    training_dataset_folder, \n",
    "    training_dataset_filename,\n",
    ")\n",
    "\n",
    "# Create the enclosing directory if not existent already.\n",
    "os.makedirs(training_dataset_folder, exist_ok=True)\n",
    "\n",
    "# Dictionaries for shell and core properties. Dimensions are set with lambda\n",
    "# functions to introduce a bounded level of randomness to the dataset. \n",
    "sphere_properties = {\n",
    "    \"radius\": lambda: np.random.uniform(0.7, 1.2) * particle_radius * dt.units.nm,\n",
    "    \"intensity\": lambda: np.random.uniform(0.7, 1.3) * 5E4,\n",
    "    }\n",
    "\n",
    "# Set the optical properties of the microscope.  \n",
    "optics_properties = dt.Fluorescence(\n",
    "    NA=0.5,  # Numerical aperture.\n",
    "    wavelength=370 * dt.units.nm,\n",
    "    refractive_index_medium=1.33,\n",
    "    output_region=[0, 0, image_size, image_size],\n",
    "    magnification=1,\n",
    "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
    ")\n",
    "\n",
    "# Try to load preexisting simulated data; if not available or forced, raise the\n",
    "# exception error to generate new data.\n",
    "try:\n",
    "    if force_simulation: \n",
    "        # Raise the exception error if simulation is forced.\n",
    "        raise FileNotFoundError(\"Forced simulation by user request.\")\n",
    "    \n",
    "    if not os.path.isfile(training_dataset_filepath):\n",
    "        # If file is not found, start training.    \n",
    "        raise FileNotFoundError(\"Training dataset file not found. Starting simulation.\")\n",
    "    \n",
    "    # Load existing data\n",
    "    data = np.load(training_dataset_filepath)\n",
    "    crop_images = data[\"crop_images\"]\n",
    "    crop_maps = data[\"crop_maps\"]\n",
    "    Nsamples = len(crop_images)  # Update number of samples.\n",
    "    print(f\"Loaded file: {training_dataset_filepath}\")\n",
    "        \n",
    "# Handle the case of either file not found or forced training.\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # Perform simulation if file not found or forced.\n",
    "    crop_images, crop_maps = utils.generate_particle_dataset(\n",
    "        num_samples,\n",
    "        image_size,\n",
    "        max_num_particles,\n",
    "        sphere_properties,\n",
    "        optics_properties=optics_properties,\n",
    "    )\n",
    "\n",
    "    # Save the simulated training dataset.\n",
    "    np.savez(\n",
    "        training_dataset_filepath, \n",
    "        crop_images=crop_images, \n",
    "        crop_maps=crop_maps,\n",
    "    )\n",
    "    print(\"New dataset generated and saved.\")\n",
    "    \n",
    "# Plot up to 18 simulated crops.\n",
    "utils.plot_crops(np.squeeze(crop_images), title=\"Simulated Crops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928bae3",
   "metadata": {},
   "source": [
    "Build a training pipeline that randomly selects and augments particle crops on the fly. Each crop is chosen randomly from the simulated dataset and normalized with slightly varying min/max values to mimic contrast and illumination changes.\n",
    "\n",
    "This setup creates a diverse training set without needing to store multiple augmented copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training pipeline with additional settings, such as multiplication\n",
    "# of global intensity and Gaussian noise.\n",
    "selector = dt.Lambda(\n",
    "    lambda i: lambda x: x[i], i=lambda l: np.random.randint(l), l=len(crop_images)\n",
    ")\n",
    "\n",
    "# Define a training pipeline with augmentations. \n",
    "training_pipeline = (\n",
    "    dt.Value(crop_images)\n",
    "    #>> dt.Gaussian(0, 0.025)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), np.random.uniform(0.8, 1.0)\n",
    "    )\n",
    "    >> selector\n",
    ")\n",
    "\n",
    "# Build a training dataset with a pipeline that randomly selects a crop each\n",
    "# time.\n",
    "train_dataset = dt.pytorch.Dataset(\n",
    "    training_pipeline,\n",
    "    length=256,\n",
    ")\n",
    "\n",
    "# Dataloader contains the randomized training dataset and additional settings.\n",
    "dataloader = dl.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# We can check one of the elements in the training pipeline with augmentations.\n",
    "sanity_check_crop = training_pipeline.update().resolve()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(sanity_check_crop), cmap=\"gray\", aspect=\"equal\")\n",
    "plt.title(\"Random Crop from Training Pipeline\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0e6a4",
   "metadata": {},
   "source": [
    "### Train LodeSTAR and Apply it to Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab967be",
   "metadata": {},
   "source": [
    "Create a LodeSTAR model with 4 random geometric transformations per input and set up the trainer, which will handle the training loop. It runs for 32 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the model.\n",
    "lodestar = dl.LodeSTAR(\n",
    "    n_transforms=4, \n",
    "    optimizer=dl.Adam(lr=2e-4), \n",
    ").build()\n",
    "\n",
    "# Set up the trainer and specify number of epochs.\n",
    "trainer_lodestar = dl.Trainer(max_epochs=32, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaae69",
   "metadata": {},
   "source": [
    "Check if pretrained weights already exist. Otherwise train LodeSTAR on the simulated crops using the training pipeline and save the trained weights for later use. Otherwise, simply load the existing weights from file.\n",
    "\n",
    "Finally, switch the model to evaluation mode, which disables training-specific behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether to force training even if weights exist.\n",
    "force_training = True\n",
    "\n",
    "# Define folder and path for storing LodeSTAR weights.\n",
    "folder_name = \"LodeSTAR\"\n",
    "lodestar_path = os.path.join(folder_name, \"lodestar_weights_simulated\")\n",
    "\n",
    "# Train or load weights.\n",
    "if not os.path.isfile(lodestar_path) or force_training:\n",
    "    print(\"Training LodeSTAR model (either forced or weights not found).\")\n",
    "    \n",
    "    # Ensure the save directory exists.\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    # Train the model.\n",
    "    trainer_lodestar.fit(lodestar, dataloader)\n",
    "    \n",
    "    # Save weights.\n",
    "    torch.save(lodestar.state_dict(), lodestar_path)\n",
    "    print(f\"Saved LodeSTAR weights to '{lodestar_path}'.\")\n",
    "else:\n",
    "    # Load pre-trained weights.\n",
    "    lodestar.load_state_dict(torch.load(lodestar_path, weights_only=True))\n",
    "    print(f\"Loaded preexisting LodeSTAR weights from '{lodestar_path}'.\")\n",
    "\n",
    "# Set model to evaluation mode.\n",
    "lodestar.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06394103",
   "metadata": {},
   "source": [
    "Apply the trained LodeSTAR model to a simulated image.  Set the inference parameters which control detection sensitivity. Get the prediction features, including the mass_feature, which can be useful for visualizing detections. Extract the final coordinates of the detected particles and print how many particles were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for inference.\n",
    "alpha = 0.09\n",
    "beta = 1 - alpha\n",
    "cutoff = 0.001\n",
    "\n",
    "# Extract predictions.\n",
    "prediction = lodestar(image_of_particles_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction.\n",
    "mass_feature = prediction[2]\n",
    "\n",
    "# Infer on the pytorch image.\n",
    "detections_lodestar = lodestar.detect(\n",
    "    image_of_particles_tensor,\n",
    "    alpha = alpha,\n",
    "    beta = beta, \n",
    "    mode = \"constant\",\n",
    "    cutoff = cutoff,\n",
    ")[0]\n",
    "                         \n",
    "print(f\"Found {len(detections_lodestar[:, 1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922d4e2",
   "metadata": {},
   "source": [
    "Display mass features predicted for the simulated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485466b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(simulated_image), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Simulated Image\")\n",
    "\n",
    "#  Subfigure 2, mass feature distribution.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass Features Predicted by LodeSTAR\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982c9bb",
   "metadata": {},
   "source": [
    "Plot the predicted positions from LodeSTAR alongside the ground truth on the simulated image and quantify the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted positions and the ground truth overlaid on the simulated\n",
    "# image.\n",
    "utils.plot_predicted_positions(\n",
    "    image=simulated_image,\n",
    "    predicted_positions=detections_lodestar,\n",
    "    ground_truth_positions=gt_pos,\n",
    "    title=\"Method 4 Trained on Simulations - Simulated Image\",\n",
    ")\n",
    "#  Evaluate performance.\n",
    "utils.evaluate_locs(detections_lodestar, gt_pos, distance_th=particle_radius);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c7ea3",
   "metadata": {},
   "source": [
    "### Apply the Trained LodeSTAR on Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bf671",
   "metadata": {},
   "source": [
    "- Apply LodeSTAR to the experimental image using the same inference settings (alpha, beta, cutoff) as before.\n",
    "- Run the model to extract prediction features, including the mass channel.\n",
    "- Get the final particle coordinates and print the total number of detections.\n",
    "- Visualize the original experimental image and the mass feature map predicted by LodeSTAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameters for inference.\n",
    "alpha = 0.09\n",
    "beta = 1 - alpha\n",
    "cutoff = 0.001\n",
    "\n",
    "#  Extract predictions and convert into a tensor\n",
    "prediction_exp = lodestar(image_of_particles_exp_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction\n",
    "mass_feature_exp = prediction_exp[2]\n",
    "\n",
    "#  Infer positions from experimental image.\n",
    "detections_lodestar_experimental = lodestar.detect(\n",
    "    image_of_particles_exp_tensor,\n",
    "    alpha = alpha,\n",
    "    beta = beta, \n",
    "    mode = \"constant\",\n",
    "    cutoff = cutoff,\n",
    ")[0]\n",
    "                         \n",
    "### Display predictions of experimental image\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(image_of_particles_exp), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Experimental Image\")\n",
    "\n",
    "#  Subfigure 2, mass features.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature_exp, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass Features Predicted by LodeSTAR\")\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51698468",
   "metadata": {},
   "source": [
    "Display the experimental image with the predicted particle positions overlaid to visually inspect the detection quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6075dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predicted_positions(\n",
    "   image=image_of_particles_exp[0, 0, :, :],\n",
    "   predicted_positions= detections_lodestar_experimental,\n",
    "   title=\"Method 4 Trained on Simulations - Experimental Image\",\n",
    ")\n",
    "print(f\"Found {len(detections_lodestar_experimental[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f4fd9",
   "metadata": {},
   "source": [
    "### Train LodeSTAR with Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa7ee9",
   "metadata": {},
   "source": [
    "Since LodeSTAR is self-supervised, you can train it directly on experimental crops—no ground-truth labels needed.\n",
    "\n",
    "Start by identifying a few representative particles in the experimental image. From these, extract small crops centered on the particles. These serve as the training data, and LodeSTAR learns by applying random transformations to these crops and predicting how the particle positions change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates from relevant crop samples.\n",
    "xs = [230, 168, 362, 330]\n",
    "ys = [272, 177, 190, 435]\n",
    "number_of_crops = len(xs)\n",
    "\n",
    "# Pre-allocate array to store all the crops.\n",
    "training_images = []\n",
    "for i in range(number_of_crops):\n",
    "    # Go through the locations of the crops in the image.\n",
    "    y_index = ys[i]\n",
    "    x_index = xs[i]\n",
    "    crop_size = 25\n",
    "\n",
    "    # Crop a window in the image.\n",
    "    x0 = x_index\n",
    "    y0 = y_index\n",
    "    training_image = np.array(image[x0:x0 + crop_size, y0: y0 + crop_size])\n",
    "\n",
    "    # Expand dims with np.newaxis and append to list.\n",
    "    training_images.append(training_image[np.newaxis, ...])\n",
    "\n",
    "# Plot up to 18 simulated crops.\n",
    "utils.plot_crops(np.squeeze(training_images), title=\"Experimental Crops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965ddf1",
   "metadata": {},
   "source": [
    "Prepare the training pipeline with the selected experimental crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random selector of crops.\n",
    "random_crop = lambda: random.choice(training_images)\n",
    "\n",
    "# Define training pipeline.\n",
    "training_pipeline = (\n",
    "    dt.Value(random_crop)\n",
    "    >> dt.NormalizeMinMax(\n",
    "        lambda: np.random.uniform(0.0, 0.1), \n",
    "        lambda: np.random.uniform(0.85, 1.0),\n",
    "        )\n",
    "    # >> dt.Gaussian(0.0, 0.01)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Build a training dataset with a pipeline that randomly selects a crop each\n",
    "# time.\n",
    "train_dataset = dt.pytorch.Dataset(\n",
    "    training_pipeline,\n",
    "    length=512,\n",
    ")\n",
    "\n",
    "# Dataloader contains the randomized training dataset and additional settings.\n",
    "dataloader = dl.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "# We can check one of the elements in the training pipeline with augmentations.\n",
    "sanity_check_crop = training_pipeline.update()()\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(sanity_check_crop), cmap=\"gray\", aspect=\"equal\")\n",
    "plt.title(\"Random Crop from Training Pipeline\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881f6b0",
   "metadata": {},
   "source": [
    "Create a LodeSTAR model with 8 random geometric transformations per input and set up the trainer, which will handle the training loop. It runs for 32 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the model.\n",
    "lodestar_exp = dl.LodeSTAR(\n",
    "    n_transforms=8, \n",
    "    optimizer=dl.Adam(lr=1e-3),\n",
    ").build()\n",
    "\n",
    "# Set up the trainer and specify number of epochs.\n",
    "trainer_lodestar_exp = dl.Trainer(max_epochs=32, accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ab50c",
   "metadata": {},
   "source": [
    "Check if pretrained weights already exist. Otherwise train LodeSTAR on the experimental crops using the training pipeline and save the trained weights for later use. Otherwise, simply load the existing weights from file.\n",
    "\n",
    "Finally, switch the model to evaluation mode, which disables training-specific behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether to force training even if weights exist.\n",
    "force_training = True\n",
    "\n",
    "# Define folder and path for storing LodeSTAR experimental weights.\n",
    "folder_name = \"LodeSTAR\"\n",
    "lodestar_exp_path = os.path.join(folder_name, \"lodestar_weights_exp\")\n",
    "\n",
    "# Train or load weights.\n",
    "if not os.path.isfile(lodestar_exp_path) or force_training:\n",
    "    print(\"Training LodeSTAR on experimental data (either forced or weights not found).\")\n",
    "    \n",
    "    # Ensure the save directory exists.\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Train the model.\n",
    "    trainer_lodestar_exp.fit(lodestar_exp, dataloader)\n",
    "\n",
    "    # Save weights.\n",
    "    torch.save(lodestar_exp.state_dict(), lodestar_exp_path)\n",
    "    print(f\"Saved experimental LodeSTAR weights to '{lodestar_exp_path}'.\")\n",
    "else:\n",
    "    # Load pre-trained weights.\n",
    "    lodestar_exp.load_state_dict(torch.load(lodestar_exp_path, weights_only=True))\n",
    "    print(f\"Loaded preexisting LodeSTAR weights from '{lodestar_exp_path}'.\")\n",
    "\n",
    "# Set model to evaluation mode.\n",
    "lodestar_exp.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13c1ea",
   "metadata": {},
   "source": [
    "Apply the trained LodeSTAR model to an experimental image. Set the inference parameters which control detection sensitivity. Get the prediction features, including the mass_feature, which can be useful for visualizing detections. Extract the final coordinates of the detected particles and print how many particles were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d248163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameters for inference.\n",
    "alpha = 0.05\n",
    "beta = 1 - alpha \n",
    "cutoff = 0.005\n",
    "#  Extract predictions and convert into a tensor.\n",
    "prediction_exp = lodestar_exp(image_of_particles_exp_tensor).squeeze().detach()\n",
    "\n",
    "# Extract the mass feature from the prediction.\n",
    "mass_feature_exp = prediction_exp[2]\n",
    "\n",
    "#  Infer positions from experimental image.\n",
    "detections_lodestar_experimental = lodestar_exp.detect(\n",
    "    image_of_particles_exp_tensor,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    mode=\"constant\",\n",
    "    cutoff=cutoff,\n",
    ")[0]\n",
    "                         \n",
    "print(f\"Found {len(detections_lodestar_experimental[:,1])} detections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabc72e",
   "metadata": {},
   "source": [
    "Display mass features predicted for the experimental image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348473d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size.\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "#  Subfigure 1, test image.\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(image_of_particles_exp), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Experimental Image\")\n",
    "\n",
    "#  Subfigure 2, mass feature distribution.\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mass_feature_exp, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mass Features Predicted by LodeSTAR\")\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecc660",
   "metadata": {},
   "source": [
    "Plot the predicted positions from LodeSTAR on the experimental image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cddcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predicted_positions(\n",
    "   image=image_of_particles_exp[0, 0, :, :],\n",
    "   predicted_positions= detections_lodestar_experimental,\n",
    "   title=\"Method 4 Trained on Experimental Crops - Experimental Image\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
