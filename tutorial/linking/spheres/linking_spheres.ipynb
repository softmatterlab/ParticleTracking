{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aarondomenzain/tracking-softmatter-aarond/blob/tracking-softmatter-aarond-dev/tutorial/tracking/tracking_spheres.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hcsWD9QOZQ"
      },
      "source": [
        "# Particle Tracking Tutorial: Trajectory Linking\n",
        "In this tutorial, you’ll explore different methods to link particle localizations across time to recostruct trajectories — using both simulated data and real experimental images.\n",
        "\n",
        "You’ll start by generating a simulated movie of microscopic particles undergoing Brownian motion, mimicking what you might see in a soft matter or biophysics experiment. For each frame, you'll use LodeSTAR, a self-supervised neural network, to detect and localize particles. Then comes the core challenge: linking localization into trajectories.\n",
        "\n",
        "Here’s what you’ll test and compare:\n",
        "\n",
        "- Nearest-neighbor linking (using TrackPy — a classic in particle tracking)\n",
        "\n",
        "- Linear Assignment Problem (LAP) (using LapTrack - a more flexible and general framework)\n",
        "\n",
        "- MAGIK (a geometric deep learning method based on graph neural networks)\n",
        "\n",
        "You’ll be using Python libraries like NumPy, SciPy, Matplotlib, scikit-image, PyTorch, DeepTrack, and Deeplay. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "0. [Importing the Required Libraries and Loading Utility Functions](#importing-the-required-libraries-and-loading-utility-functions)\n",
        "1. [Loading and Visualizing Experimental Videos](#loading-and-visualizing-experimental-videos)\n",
        "2. [Simulating Realistic Videos with DeepTrack](#simulating-realistic-videos-with-deeptrack)\n",
        "    - [Simulating a Single Particle](#simulating-a-single-particle)\n",
        "    - [Simulating a Video Frame](#simulating-a-video-frame)\n",
        "    - [Simulating Brownian Trajectories](#simulating-brownian-trajectories)\n",
        "    - [Simulating a Video](#simulating-a-video)\n",
        "\n",
        "2. [Detecting and Localizing Particles with LodeSTAR](#detecting-and-localizing-particles-with-lodestar)\n",
        "    - [Training LodeSTAR with Experiments](#training-lodestar-with-experiments)\n",
        "    - [Evaluating LodeSTAR on Simulations](#evaluating-lodestar-on-simulations)\n",
        "    - [Applying LodeSTAR to Simulations](#applying-lodestar-to-simulations)\n",
        "    - [Applying LodeSTAR to Experiments](#applying-lodestar-to-experiments)\n",
        "\n",
        "3. [Method 1: Nearest-neighbor Linking with TrackPy](#method-1-nearest-neighbor-linking-with-trackpy)\n",
        "    - [Linking Localizations in Simulations](#linking-localizations-in-simulations)\n",
        "    - [Evaluating Linking Performance](#evaluating-linking-performance)\n",
        "    - [Linking Localizations in Experiments](#linking-localizations-in-experiments)\n",
        "\n",
        "4. [Method 2: Linear Assignment Problem (LAP) with LapTrack](#method-2-linear-assignment-problem-lap-with-laptrack)\n",
        "    - [Linking Localizations in Simulations](#linking-localizations-in-simulations)\n",
        "    - [Evaluating Linking Performance](#evaluating-linking-performance)\n",
        "    - [Linking Localizations in Experiments](#linking-localizations-in-experiments)\n",
        "\n",
        "5. [Method 3: MAGIK](#method-3-magik)\n",
        "    - [Training MAGIK with Simulations](#training-magik-with-simulations)\n",
        "    - [Linking Localizations in Simulations](#linking-localizations-in-simulations)\n",
        "    - [Evaluating Linking Performance](#evaluating-linking-performance)\n",
        "    - [Linking Localizations in Experiments](#linking-localizations-in-experiments)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the Required Libraries and Loading Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment the next cell if running on Google Colab/Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MXGmUjyD-Ve"
      },
      "outputs": [],
      "source": [
        "#!pip install deeptrack deeplay trackpy laptrack -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WadH3picQOZT"
      },
      "outputs": [],
      "source": [
        "# Standard libraries.\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# Configuration\n",
        "import matplotlib\n",
        "matplotlib.rcParams[\"animation.embed_limit\"] = 60 # Allow larger animations inline\n",
        "logging.disable(logging.WARNING) # Suppress warnings and below\n",
        "\n",
        "# Core Scientific Stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting and Display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Machine Learning\n",
        "import torch\n",
        "from torchvision.transforms import Compose\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import deeplay as dl\n",
        "\n",
        "# Particle Tracking and Simulation\n",
        "import deeptrack as dt\n",
        "import trackpy as tp\n",
        "from laptrack import LapTrack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a set of custom functions defined specifically for this notebook from the `utils` directory. For detailed documentation of each function, refer to the comments and docstrings within the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load functions and utilities for dataset generation and visualization.\n",
        "# Sys append a folder to the path.\n",
        "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
        "\n",
        "# Import all the functions contained in the folder utils.\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set random seeds to make results reproducible across runs, especially during training and data simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set a fixed seed value\n",
        "seed = 98\n",
        "\n",
        "# Python, NumPy, and PyTorch (CPU)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Only set CUDA seeds if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"Seeds set to {seed} (with CUDA: {torch.cuda.is_available()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lhy7J2ePRQw"
      },
      "source": [
        "## Loading and Visualizing Experimental Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2MF8ohRFKZV"
      },
      "source": [
        "You'll use experimental video of a system of colloidal particles recorded with fluorescence microscopy. Data from https://www.nature.com/articles/s41467-022-30497-z.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "ico6y7gkFKZV",
        "outputId": "752d216d-df5b-4397-a552-9106b11375d2"
      },
      "outputs": [],
      "source": [
        "# Define the folder and video file name.\n",
        "video_folder = \"videos\"\n",
        "video_file_name = \"experimental_video.npy\"\n",
        "\n",
        "# Construct the full path.\n",
        "video_path = os.path.join(video_folder, video_file_name)\n",
        "\n",
        "# Load the video data.\n",
        "exp_video = np.load(video_path)\n",
        "\n",
        "utils.play_video(exp_video, \"Experimental Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the first frame of the video, manually select and display a single particle by specifying its centroid coordinates (x, y) and a box width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the first frame of the video.\n",
        "exp_image = exp_video[0]\n",
        "\n",
        "# Get the shape of the image.\n",
        "assert exp_image.shape[0] == exp_image.shape[1], \"Warning: Image is not square!\"\n",
        "exp_image_size = exp_image.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify the parameters for the viewing box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box size to zoom in an individual particle.\n",
        "exp_crop_size = 15\n",
        "\n",
        "# Coordinates of the center of the particle.\n",
        "x_center = 96\n",
        "y_center = 41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate top-left corner of the crop.\n",
        "x = x_center - exp_crop_size // 2\n",
        "y = y_center - exp_crop_size // 2\n",
        "\n",
        "# Select a crop as a subset of the entire image.\n",
        "exp_crop = exp_image[y:y + exp_crop_size, x:x + exp_crop_size]  # row (y), column (x)\n",
        "\n",
        "# Initialize figure instance.\n",
        "fig = plt.figure()\n",
        "\n",
        "vmin, vmax = np.percentile(exp_image, [1, 99])\n",
        "\n",
        "# Draw a red rectangle around the crop.\n",
        "fig.add_subplot(111)\n",
        "plt.imshow(exp_image, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.title(\"Experimental Image\", size=13)\n",
        "plt.plot([x, x, x+exp_crop_size, x+exp_crop_size, x],[y, y+exp_crop_size, y+exp_crop_size, y, y], 'r-')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the rectangle on the top right corner.\n",
        "fig.add_subplot(555)\n",
        "plt.imshow(exp_crop, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating Realistic Videos with DeepTrack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DeepTrack allows you to simulate physically realistic microscopy images, enabling precise control over imaging parameters and particle properties. These simulations provide ground-truth data, making them ideal for benchmarking classical and AI-based tracking methods, as well as for training neural networks in a controlled environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating a Single Particle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjust the simulation parameters to accurately replicate the features observed in the cropped region of the experimental image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same as the box width.\n",
        "sim_crop_size = exp_crop_size  \n",
        "\n",
        "# Size of a pixel in nanometers in the output image.\n",
        "pixel_size_nm = 100 # In nm.\n",
        "\n",
        "# Radius of the particle.\n",
        "particle_radius = 200 # In nm.\n",
        "\n",
        "# Define central spherical scatterer.\n",
        "sphere = dt.Sphere(\n",
        "    position=0.5 * np.array([sim_crop_size, sim_crop_size]) + (-0.5, 0.5),\n",
        "    z= 0 * dt.units.nm, # Particle in focus.\n",
        "    radius= particle_radius * dt.units.nm,  # Radius in nanometers.\n",
        "    intensity= 0.9E5,  # Field magnitude squared.\n",
        ")\n",
        "\n",
        "# Simulate the properties of the fluorescence microscope.\n",
        "optics = dt.Fluorescence(\n",
        "    NA=1.0,  # Numerical aperture.\n",
        "    wavelength=638 * dt.units.nm,\n",
        "    refractive_index_medium=1.33,\n",
        "    output_region=[0, 0, sim_crop_size, sim_crop_size],\n",
        "    magnification=1,\n",
        "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
        ")\n",
        "\n",
        "# Apply transformations.\n",
        "sim_crop = (\n",
        "    optics(sphere)\n",
        "    >> dt.Background(750)\n",
        "    >> dt.Poisson(snr=6500)\n",
        ")\n",
        "\n",
        "# Turn the crop into a NumPy array.\n",
        "sim_crop = np.squeeze(sim_crop())\n",
        "\n",
        "# Plot the simulated and experimental crops.\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "# Simulated crop.\n",
        "plot = axes[0].imshow(sim_crop, cmap=\"gray\")\n",
        "axes[0].axis(\"off\")\n",
        "axes[0].set_title(\"Simulated Crop\")\n",
        "\n",
        "# Experimental crop.\n",
        "axes[1].imshow(exp_crop, cmap=\"gray\")  \n",
        "axes[1].axis(\"off\")\n",
        "axes[1].set_title(\"Experimental Crop\")\n",
        "\n",
        "# Adjust layout and show plot.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract and visualize the raw intensity profiles along the central horizontal and vertical lines of both the simulated and experimental crops. This comparison helps evaluate how well the simulation reproduces the intensity distribution observed in real microscopy images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the center index.\n",
        "center = sim_crop_size // 2\n",
        "\n",
        "# Extract horizontal (row) profiles.\n",
        "sim_horiz = sim_crop[center, :]\n",
        "exp_horiz = exp_crop[center, :]\n",
        "\n",
        "# Extract vertical (column) profiles.\n",
        "sim_vert = sim_crop[:, center]\n",
        "exp_vert = exp_crop[:, center]\n",
        "\n",
        "# Create a 1×2 subplot.\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), tight_layout=True)\n",
        "\n",
        "# --- Horizontal profile ---\n",
        "axes[0].plot(sim_horiz, label=\"Simulated Crop\", color=\"orange\")\n",
        "axes[0].plot(exp_horiz, label=\"Experimental Crop\", color=\"blue\")\n",
        "axes[0].set_xlabel(\"Pixel (x)\")\n",
        "axes[0].set_ylabel(\"Intensity\")\n",
        "axes[0].set_title(\"Horizontal Intensity Profile (Center Row)\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "# --- Vertical profile ---\n",
        "axes[1].plot(sim_vert, label=\"Simulated Crop\", color=\"orange\")\n",
        "axes[1].plot(exp_vert, label=\"Experimental Crop\", color=\"blue\")\n",
        "axes[1].set_xlabel(\"Pixel (y)\")\n",
        "axes[1].set_ylabel(\"Intensity\")\n",
        "axes[1].set_title(\"Vertical Intensity Profile (Center Column)\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating a Video Frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a simulated image containing non-overlapping spherical particles. Begin by generating their coordinates—these will serve as the ground-truth positions. Then, use DeepTrack to render optically realistic particles at these coordinates, resulting in a physically plausible microscopy image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters of the simulation.\n",
        "sim_image_size = exp_image_size\n",
        "N_particles = 60\n",
        "particle_radius = 200  # Particle radius in nm.\n",
        "\n",
        "# Dictionary for particle properties. Dimensions are set with lambda\n",
        "# functions to introduce variety to the dataset.\n",
        "sphere_properties = {\n",
        "    \"intensity\": lambda: np.random.normal(1.1, 0.3)*0.9E5,\n",
        "    \"z\": lambda: np.random.uniform(0, 3000) * dt.units.nm,\n",
        "    \"radius\": particle_radius * dt.units.nm,\n",
        "}\n",
        "\n",
        "# Set the optical properties of the microscope.  \n",
        "optics_properties = dt.Fluorescence(\n",
        "    NA=1.0,  # Numerical aperture.\n",
        "    wavelength=638 * dt.units.nm,\n",
        "    refractive_index_medium=1.33,\n",
        "    output_region=[0, 0, sim_image_size, sim_image_size],\n",
        "    magnification=1,\n",
        "    resolution=pixel_size_nm * dt.units.nm, # Camera resolution or effective resolution.\n",
        ")\n",
        "\n",
        "# Generate ground truth positions.\n",
        "sim_gt_pos = utils.generate_centroids(\n",
        "    num_particles=N_particles,\n",
        "    fov_size=sim_image_size,\n",
        "    particle_radius=particle_radius,\n",
        ")\n",
        "\n",
        "# Simulate image.\n",
        "sim_image = utils.transform_to_video(\n",
        "    sim_gt_pos,\n",
        "    fov_size=sim_image_size,\n",
        "    core_particle_props=sphere_properties,\n",
        "    optics_props=optics_properties,\n",
        "    background_props={\"poisson_snr\": 6500, \"background_mean\": 750},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the simulated image and compare it with the experimental one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the simulated and experimental images.\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "vmin, vmax = np.percentile(exp_image, [1, 99])\n",
        "\n",
        "# Simulated image.\n",
        "plot = axes[0].imshow(sim_image, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
        "axes[0].axis(\"off\")\n",
        "axes[0].set_title(\"Simulated Image\")\n",
        "\n",
        "# Experimental image.\n",
        "axes[1].imshow(exp_image, cmap=\"gray\", vmin=vmin, vmax=vmax)  \n",
        "axes[1].axis(\"off\")\n",
        "axes[1].set_title(\"Experimental Image\")\n",
        "\n",
        "# Adjust layout and show plot.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform a more quantitative comparison by plotting the intensity histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten the arrays to 1D.\n",
        "sim_vals = sim_image.ravel()\n",
        "exp_vals = exp_image.ravel()\n",
        "\n",
        "# Compute common bin edges.\n",
        "all_vals = np.concatenate([sim_vals, exp_vals])\n",
        "num_bins = 60\n",
        "bins = np.linspace(all_vals.min(), np.quantile(all_vals, 0.99), num_bins + 1)\n",
        "\n",
        "# Create figure with two subplots sharing axes.\n",
        "fig, axes = plt.subplots(\n",
        "    1, 2,\n",
        "    figsize=(10, 4),\n",
        "    sharey=True, sharex=True,\n",
        "    tight_layout=True\n",
        ")\n",
        "\n",
        "# Histogram for simulated image.\n",
        "axes[0].hist(\n",
        "    sim_vals,\n",
        "    bins=bins,\n",
        "    alpha=0.7,\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "axes[0].set_title(\"Simulated Image Histogram\")\n",
        "axes[0].set_xlabel(\"Intensity\")\n",
        "axes[0].set_ylabel(\"Pixel Count\")\n",
        "\n",
        "# Histogram for experimental image.\n",
        "axes[1].hist(\n",
        "    exp_vals,\n",
        "    bins=bins,\n",
        "    alpha=0.7,\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "axes[1].set_title(\"Experimental Image Histogram\")\n",
        "axes[1].set_xlabel(\"Intensity\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_6ZSzxmQOZU"
      },
      "source": [
        "### Simulating Brownian Trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll simulate a set of trajectories that visually resemble the experimental data to evaluate the performance of different tracking methods. The goal is to replicate the Brownian motion of nanoparticles as observed in the experimental videos. This is done using the `simulate_Brownian_trajs` function from the utility file, which generates 2D trajectories based on a random walk model. Refer to the function for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQUXCeWaQOZW"
      },
      "outputs": [],
      "source": [
        "# Simulation parameters.\n",
        "number_particles = 60\n",
        "number_timesteps = 50\n",
        "\n",
        "# Simulate trajectories for one video.\n",
        "sim_trajs_gt = utils.simulate_Brownian_trajs(\n",
        "    num_particles = number_particles,\n",
        "    num_timesteps = number_timesteps,\n",
        "    fov_size = sim_image_size,\n",
        "    diffusion_std=0.5, # Corresponds to sqrt(2Dt),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For evaluation purposes, trajectories that move out and back in the field of view due to boundary conditions are treated as separate trajectories. For further analysis, the trajectories are transformed into a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Break trajectories going in/out of FOV.\n",
        "sim_trajs_gt_list = utils.traj_break(\n",
        "    trajs = sim_trajs_gt,\n",
        "    fov_size = sim_image_size,\n",
        "    num_particles = sim_trajs_gt.shape[1],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE3jXw3MQOZX"
      },
      "source": [
        "### Simulating a Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll simulate a video of particle motion that resembles experimental data and compares the two by playing them simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Several out-of-focus particles are rather dim and not clearly visible unless the dynamic range of the video is adjusted. However, including them is essential to accurately reproduce the experimental conditions, especially in terms of intensity distribution and background noise characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gs3F2pQBQOZX",
        "outputId": "bb2c3b9d-ef96-4f90-9569-92ea4fea1a4b"
      },
      "outputs": [],
      "source": [
        "# Simulate video.\n",
        "sim_video = utils.transform_to_video(\n",
        "    np.delete(sim_trajs_gt, 2, 2), # Remove frame axis.\n",
        "    fov_size=sim_image_size,\n",
        "    core_particle_props=sphere_properties,\n",
        "    optics_props=optics_properties,\n",
        "    background_props={\"poisson_snr\": 6500, \"background_mean\": 750},\n",
        "    save_video=True,\n",
        "    path=\"videos/simulated_video.tiff\",\n",
        ")\n",
        "\n",
        "# Play both videos and compare.\n",
        "utils.play_video(sim_video, \"Simulated Video\")\n",
        "utils.play_video(exp_video, \"Experimental Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsF77OGQOZY"
      },
      "source": [
        "## Detecting and Localizing Particle with LodeSTAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LodeSTAR will be used to detect the particles' positions at each frame of the video, similarly as shown in the **Detections** notebooks of this tutorial. These positions will be passed to different linking methods to build trajectories and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training LodeSTAR with Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the training, you'll start by obtaining crops of our particles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "JBoMXJKAFKZZ",
        "outputId": "a7c8b0b6-2f6a-4883-d65b-1643090201ab"
      },
      "outputs": [],
      "source": [
        "# Define crop parameters: (x, y, width)\n",
        "crop_params = [\n",
        "    (196, 218, 20),\n",
        "    (97, 144, 20),\n",
        "    (232, 233, 20),\n",
        "    (59, 203, 20),\n",
        "    (202, 70, 20),\n",
        "]\n",
        "\n",
        "# Extract crops from the image.\n",
        "exp_crops = [exp_image[y:y + w, x:x + w] for x, y, w in crop_params]\n",
        "\n",
        "# Plot up to 18 crops.\n",
        "utils.plot_crops(np.squeeze(exp_crops), title=\"Experimental Crops\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the training pipeline with the selected experimental crops. If your crop is large (i.e larger than64x64), you can make the training process faster by using meanpooling in the training pipeline with `>> dt.Pool(np.mean, 2)`, which is added but commented out in the pipeline below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tUtpRtJQOZY"
      },
      "outputs": [],
      "source": [
        "# Random selector of crops.\n",
        "random_crop = lambda: random.choice(np.array(exp_crops))\n",
        "\n",
        "# Define training pipeline with augmentations.\n",
        "train_pipeline = (\n",
        "    dt.Value(random_crop)\n",
        "    >> dt.Affine(scale=lambda: np.random.uniform(0.8, 1.2))\n",
        "    # >> dt.Pool(np.mean, 2)\n",
        "    >> dt.NormalizeMinMax(0.0, 1.0)\n",
        "    >> dt.Gaussian(0, 0.05)\n",
        "    >> dt.NormalizeMinMax(\n",
        "        lambda: np.random.uniform(0.0, 0.1), \n",
        "        lambda: np.random.uniform(0.9, 1.0),\n",
        "        )\n",
        "    >> dt.MoveAxis(-1, 0)\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
        ")\n",
        "\n",
        "# Define dataset.\n",
        "train_dataset = dt.pytorch.Dataset(\n",
        "    train_pipeline,\n",
        "    length=512,\n",
        ")\n",
        "\n",
        "# Define Dataloader.\n",
        "dataloader = dl.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a LodeSTAR model with 4 random geometric transformations per input and set up the trainer, which will handle the training loop. It runs for 50 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initiate LodeSTAR.\n",
        "lodestar = dl.LodeSTAR(\n",
        "    n_transforms=4,\n",
        "    optimizer=dl.Adam(lr=1e-3),\n",
        ").build()\n",
        "\n",
        "# Set up the trainer and specify number of epochs.\n",
        "trainer_lodestar = dl.Trainer(max_epochs=50, accelerator=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBNEX15S5idR"
      },
      "source": [
        "Check whether pre-trained LodeSTAR weights already exist on disk. If they are missing or if training is explicitly forced, the model is trained on the experimental crops using the specified training pipeline, and the resulting weights are saved for future use.\n",
        "\n",
        "If the weights are already available and training is not forced, they are simply loaded from file.\n",
        "\n",
        "Afterward, the model is set to evaluation mode, which disables training-specific behaviors, ensuring consistent behavior during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBIHM4HFFKZa"
      },
      "outputs": [],
      "source": [
        "# Define whether to force training even if weights exist.\n",
        "force_training = True\n",
        "\n",
        "# Define folder and path for storing LodeSTAR experimental weights.\n",
        "folder_name = \"LodeSTAR\"\n",
        "lodestar_path = os.path.join(folder_name, \"lodestar_weights\")\n",
        "\n",
        "# Train or load weights.\n",
        "if not os.path.isfile(lodestar_path) or force_training:\n",
        "    print(\"Training LodeSTAR on experimental data (either forced or weights not found).\")\n",
        "    \n",
        "    # Ensure the save directory exists.\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "    # Train the model.\n",
        "    trainer_lodestar.fit(lodestar, dataloader)\n",
        "\n",
        "    # Save weights.\n",
        "    torch.save(lodestar.state_dict(), lodestar_path)\n",
        "    print(f\"Saved experimental LodeSTAR weights to '{lodestar_path}'.\")\n",
        "else:\n",
        "    # Load pre-trained weights.\n",
        "    lodestar.load_state_dict(torch.load(lodestar_path, weights_only=True))\n",
        "    print(f\"Loaded preexisting LodeSTAR weights from '{lodestar_path}'.\")\n",
        "\n",
        "# Set model to evaluation mode.\n",
        "lodestar.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrw8fRXFKZb"
      },
      "source": [
        "### Evaluating LodeSTAR on Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the trained LodeSTAR model to a frame of the simulated video.  Set the inference parameters which control detection sensitivity. Get the prediction features, which can be useful for visualizing detections. Extract the final coordinates of the detected particles and print how many particles were detected. Plot the predicted localizations from LodeSTAR alongside the ground truth on the simulated image and quantify the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Although the performance metrics might appear low, this is expected. LodeSTAR accurately detects all particles that are in focus and visually similar to the ones used for training. However, the simulated video includes many out-of-focus particles, which are difficult to observe unless the image’s dynamic range is carefully adjusted. These blurred particles are not detected because their appearance differs significantly from the in-focus training crops, and they were not included in LodeSTAR’s training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "irPMdWcLc75g",
        "outputId": "d30c5e20-1d0a-45e2-8073-f185c6e027ed"
      },
      "outputs": [],
      "source": [
        "# select frame number\n",
        "frame_index = 0\n",
        "\n",
        "# Format according to the (N,C,X,Y) convention.\n",
        "sim_frame_formatted = utils.format_image(sim_video[frame_index])\n",
        "\n",
        "# Parameters for inference.\n",
        "alpha = 0.01\n",
        "beta = 1 - alpha\n",
        "cutoff = 0.01\n",
        "\n",
        "# Get localizations from experimental image.\n",
        "sim_locs_pred = lodestar.detect(\n",
        "    sim_frame_formatted,\n",
        "    alpha=alpha,\n",
        "    beta=beta,\n",
        "    mode=\"constant\",\n",
        "    cutoff=cutoff,\n",
        ")[0]\n",
        "\n",
        "# Print detection number for reference.\n",
        "print(f\"Found {len(sim_locs_pred[:,1])} detections.\")\n",
        "\n",
        "utils.plot_predicted_positions(\n",
        "   image=sim_frame_formatted[0, 0, :, :],\n",
        "   pred_positions= sim_locs_pred,\n",
        "   gt_positions=sim_trajs_gt[frame_index],\n",
        "   title=\"Lodestar Trained on Experimental Crops - Simulated Image\",\n",
        ")\n",
        "\n",
        "#  Evaluate performance.\n",
        "utils.evaluate_locs(sim_locs_pred, sim_trajs_gt[frame_index], distance_th=particle_radius);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applying LodeSTAR to Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYeKnrt5rHXJ"
      },
      "source": [
        "Iteratively apply LodeSTAR to every frame of the simulated video and store localizations in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDl4sN3uQOZZ",
        "outputId": "b119c015-cba7-4f60-ee28-d4c71e5e6df3"
      },
      "outputs": [],
      "source": [
        "# Define a dataframe to store all the LodeSTAR detections.\n",
        "df_sim_video = []\n",
        "for frame_index, sim_frame in enumerate(sim_video):\n",
        "\n",
        "    # Format image for LodeSTAR: shape must be (N, C, X, Y).\n",
        "    sim_frame_formatted = utils.format_image(sim_frame)\n",
        "\n",
        "    # Get detections from LodeSTAR.\n",
        "    sim_locs_pred = lodestar.detect(\n",
        "        sim_frame_formatted,\n",
        "        alpha=alpha,\n",
        "        beta=beta,\n",
        "        cutoff=cutoff,\n",
        "        mode=\"constant\",\n",
        "    )[0]\n",
        "\n",
        "    # Store detections in a DataFrame.\n",
        "    df_frame = pd.DataFrame(sim_locs_pred, columns=[\"x\", \"y\"])\n",
        "    df_frame[\"frame\"] = frame_index\n",
        "    df_sim_video.append(df_frame)\n",
        "\n",
        "    # Print no. of detections every 10 frames.\n",
        "    if frame_index % 10 == 0:\n",
        "        print(f\"Detections in frame {frame_index}: {len(sim_locs_pred)}\")\n",
        "\n",
        "# Combine all detections into a single DataFrame.\n",
        "df_sim_video = pd.concat(df_sim_video, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applying LodeSTAR to Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Iteratively apply LodeSTAR to every frame of the experimental video and store localizations in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a dataframe to store all the LodeSTAR detections.\n",
        "df_exp_video = []\n",
        "for frame_index, exp_frame in enumerate(exp_video):\n",
        "\n",
        "    # Format image for LodeSTAR: shape must be (N, C, X, Y).\n",
        "    exp_frame_formatted = utils.format_image(exp_frame)\n",
        "\n",
        "    # Get detections from LodeSTAR.\n",
        "    exp_locs_pred = lodestar.detect(\n",
        "        exp_frame_formatted,\n",
        "        alpha=alpha,\n",
        "        beta=beta,\n",
        "        cutoff=cutoff,\n",
        "        mode=\"constant\",\n",
        "    )[0]\n",
        "\n",
        "    # Store detections in a DataFrame.\n",
        "    df_frame = pd.DataFrame(exp_locs_pred, columns=[\"x\", \"y\"])\n",
        "    df_frame[\"frame\"] = frame_index\n",
        "    df_exp_video.append(df_frame)\n",
        "\n",
        "    # Print no. of detections every 10 frames.\n",
        "    if frame_index % 10 == 0:\n",
        "        print(f\"Detections in frame {frame_index}: {len(exp_locs_pred)}\")\n",
        "\n",
        "# Combine all detections into a single DataFrame.\n",
        "df_exp_video = pd.concat(df_exp_video, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oMlJeXkQOZZ"
      },
      "source": [
        "## Method 1: Nearest-Neighbor Linking with TrackPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TrackPy constructs trajectories by linking localized positions across frames using a predictive nearest-neighbor algorithm. The input must be a pandas DataFrame containing the particle positions, usually with columns `x`, `y`, and `frame`.\n",
        "\n",
        "See the [TrackPy tutorial on prediction and linking](https://soft-matter.github.io/trackpy/dev/tutorial/prediction.html) for more details on how the algorithm works and how to tune parameters like `search_range` and `memory`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linking Localizations in Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the method to the localization dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8DHTIAUQOZZ",
        "outputId": "01b92ec1-c2dc-4d78-dc25-11cb62ae8ce5"
      },
      "outputs": [],
      "source": [
        "# Link detections across frames into trajectories using trackpy.link().\n",
        "# The `search_range` parameter sets the maximum allowed displacement (in pixels)\n",
        "# between frames, and `memory` allows particles to vanish for a given number\n",
        "# of frames and still be linked to the same trajectory.\n",
        "sim_trajs_pred_method1 = tp.link(\n",
        "    df_sim_video,\n",
        "    search_range=20,\n",
        "    memory=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alwnOK-jQOZa"
      },
      "source": [
        "Create a trajectory list from the output dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwni1QQXQOZa"
      },
      "outputs": [],
      "source": [
        "sim_trajs_pred_method1_list = []\n",
        "for i in sim_trajs_pred_method1.particle.drop_duplicates():\n",
        "    traj = sim_trajs_pred_method1.loc[sim_trajs_pred_method1.particle == i,\n",
        "     [\"frame\", \"x\", \"y\"]].values\n",
        "    sim_trajs_pred_method1_list.append(traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vZUMzWYQOZa"
      },
      "source": [
        "Create a video with overlayed localizations and trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_video_method1_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = sim_trajs_pred_method1_list,\n",
        "    video = sim_video,\n",
        "    fov_size = sim_image_size,\n",
        "    trajs_gt_list = sim_trajs_gt_list,\n",
        ")\n",
        "\n",
        "# Display the video.\n",
        "sim_video_method1_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzFm5YceQOZb"
      },
      "source": [
        "### Evaluating Linking Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the overall performance of the tracking (detection + linking) method using the following metrics:\n",
        "\n",
        "- **TP (True Positives):** Number of ground-truth particles correctly matched to estimated positions.\n",
        "\n",
        "- **FP (False Positives):** Number of estimated particles that do not correspond to any ground-truth particle.\n",
        "\n",
        "- **FN (False Negatives):** Number of ground-truth particles that were not matched to any estimated position.\n",
        "\n",
        "- **α:** A measure of the overall agreement between ground-truth and estimated tracks, ignoring unmatched (spurious) estimated tracks.\n",
        "\n",
        "- **β:** A stricter version of α that penalizes unmatched (spurious) tracks, providing a more realistic performance score.  \n",
        "\n",
        "See the detailed definitions in [Chenouard et al., Nature Methods, 2014](https://www.nature.com/articles/nmeth.2808).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk-MJ37YQOZb",
        "outputId": "6406de28-a117-4201-981e-7a5c9b71145a"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance metrics.\n",
        "utils.trajectory_metrics(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method1_list,\n",
        "    eps=5,\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOwhbz58QOZc"
      },
      "source": [
        "Display the reconstructed trajectories together with the groud truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "sCM3GhhBQOZf",
        "outputId": "103e1986-3966-467e-893b-a1a24517ce66"
      },
      "outputs": [],
      "source": [
        "#  Compute the total squared distance between all trajectories to match\n",
        "#  predicted trajectories with ground truth.\n",
        "matched_pairs, _, _ = utils.trajectory_assignment(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method1_list,\n",
        "    eps=5,\n",
        ")\n",
        "\n",
        "# Plot the trajectories.\n",
        "utils.plot_trajectory_matches(sim_trajs_gt_list, sim_trajs_pred_method1_list, matched_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for all the trajectories and compare curves obtained for matching trajectories (same color)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = sim_trajs_pred_method1_list,\n",
        "    trajs_gt = sim_trajs_gt_list,\n",
        "    matched_pairs = matched_pairs,\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linking Localizations in Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the same steps to track the experiment and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Link detections across frames into trajectories using trackpy.link().\n",
        "exp_trajs_pred_method1 = tp.link(\n",
        "    df_exp_video,\n",
        "    search_range=20, \n",
        "    memory=3,\n",
        ")\n",
        "\n",
        "# Create a list to store trajectories.\n",
        "exp_trajs_pred_method1_list = []\n",
        "for i in exp_trajs_pred_method1.particle.drop_duplicates():\n",
        "    traj = exp_trajs_pred_method1.loc[exp_trajs_pred_method1.particle == i,\n",
        "     [\"frame\", \"x\", \"y\"]].values\n",
        "    exp_trajs_pred_method1_list.append(traj)\n",
        "\n",
        "\n",
        "exp_video_method1_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = exp_trajs_pred_method1_list,\n",
        "    video = exp_video,\n",
        "    fov_size = exp_image_size,\n",
        ")\n",
        "\n",
        "# Display the video.\n",
        "exp_video_method1_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for the trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = exp_trajs_pred_method1_list,\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRf0iHOZQOZf"
      },
      "source": [
        "## Method 2: Linear Assignment Problem (LAP) with LapTrack\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LapTrack solves the trajectory linking problem by formulating it as a Linear Assignment Problem (LAP), a well-established optimization approach in multi-object tracking. It builds a cost matrix that quantifies dissimilarity—typically based on spatial distance—between particle detections in consecutive frames. Lower distances correspond to lower costs and indicate higher likelihoods of correspondence.\n",
        "\n",
        "LapTrack uses the Hungarian algorithm to solve this assignment problem efficiently, minimizing the total cost across the matrix. This allows it to determine the globally optimal set of assignments across frames, enabling robust trajectory reconstruction even under challenging conditions such as high particle density or noisy detections.\n",
        "\n",
        "Examples and tutorials using LapTrack are available in the [LapTrack documentation](https://github.com/yfukai/laptrack/tree/main/docs/examples)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uo-448OQOZf"
      },
      "source": [
        "### Linking Localizations in Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the method to the localization dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "4mDW4fARQOZf",
        "outputId": "66a9f87a-1447-487a-b158-778243f5cbd5"
      },
      "outputs": [],
      "source": [
        "# Set a maximum distance in pixel units\n",
        "laptrack = LapTrack(\n",
        "    track_cost_cutoff=20,\n",
        "    gap_closing_max_frame_count=2,\n",
        ")\n",
        "\n",
        "sim_trajs_pred_method2, _, _ = laptrack.predict_dataframe(\n",
        "    df_sim_video,\n",
        "    [\"x\", \"y\"],\n",
        "    only_coordinate_cols=True,\n",
        ")\n",
        "\n",
        "sim_trajs_pred_method2 = sim_trajs_pred_method2.reset_index()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXP-woQlQOZf"
      },
      "source": [
        "Create a trajectory list from the output dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqc3NWqhQOZg"
      },
      "outputs": [],
      "source": [
        "sim_trajs_pred_method2_list=[]\n",
        "for i in sim_trajs_pred_method2.track_id.drop_duplicates():\n",
        "    traj = sim_trajs_pred_method2.loc[sim_trajs_pred_method2.track_id == i,\n",
        "     [\"frame\", \"x\", \"y\"] ].values\n",
        "    sim_trajs_pred_method2_list.append(traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caLZE0LCQOZg"
      },
      "source": [
        "Create a video with overlayed localizations and trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "yPjFBBGwQOZg",
        "outputId": "16b4baaf-8dca-4a9b-db57-fdc1c8e80350"
      },
      "outputs": [],
      "source": [
        "sim_video_method2_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = sim_trajs_pred_method2_list,\n",
        "    video = sim_video,\n",
        "    fov_size = sim_image_size,\n",
        "    trajs_gt_list = sim_trajs_gt_list,\n",
        ")\n",
        "\n",
        "# Display the video.\n",
        "sim_video_method2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpEBsU6kQOZg"
      },
      "source": [
        "### Evaluating Linking Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the overall performance of the tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwU4hSwWQOZh",
        "outputId": "2618676e-40b2-413d-d75f-17c25c95562a"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance metrics.\n",
        "utils.trajectory_metrics(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method2_list,\n",
        "    eps=5,\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the reconstructed trajectories together with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "R7cu60ZWFKZm",
        "outputId": "43c30302-d3df-4959-dcd9-9a12e8dfaaf7"
      },
      "outputs": [],
      "source": [
        "#  Compute the total squared distance between all trajectories to match\n",
        "#  predicted trajectories with ground truth.\n",
        "matched_pairs, _, _ = utils.trajectory_assignment(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method2_list,\n",
        "    eps=5,\n",
        ")\n",
        "\n",
        "# Plot the trajectories.\n",
        "utils.plot_trajectory_matches(sim_trajs_gt_list, sim_trajs_pred_method2_list, matched_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for all the trajectories and compare curves obtained for matching trajectories (same color)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = sim_trajs_pred_method2_list,\n",
        "    trajs_gt = sim_trajs_gt_list,\n",
        "    matched_pairs = matched_pairs,\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linking Localizations in Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the same steps to track the experiment and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Link detections across frames into trajectories using laptrack.\n",
        "exp_trajs_pred_method2, _, _ = laptrack.predict_dataframe(\n",
        "    df_exp_video,\n",
        "    [\"x\", \"y\"],\n",
        "    only_coordinate_cols=True,\n",
        ")\n",
        "\n",
        "exp_trajs_pred_method2 = exp_trajs_pred_method2.reset_index()\n",
        "\n",
        "\n",
        "# Create a list to store trajectories.\n",
        "exp_trajs_pred_method2_list=[]\n",
        "for i in exp_trajs_pred_method2.track_id.drop_duplicates():\n",
        "    traj = exp_trajs_pred_method2.loc[exp_trajs_pred_method2.track_id == i,\n",
        "     [\"frame\", \"x\", \"y\"] ].values\n",
        "    exp_trajs_pred_method2_list.append(traj)\n",
        "\n",
        "\n",
        "exp_video_method2_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = exp_trajs_pred_method2_list,\n",
        "    video = exp_video,\n",
        "    fov_size = exp_image_size,\n",
        ")\n",
        "\n",
        "# Display the video.\n",
        "exp_video_method2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for the trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = exp_trajs_pred_method2_list,\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r45j5Z2LQOZh"
      },
      "source": [
        "## Method 3: MAGIK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MAGIK is a tracking framework designed to analyze the motion of dynamic systems, including cells, bacteria, individual molecules, colloids, and other active particles. The name stands for Motion Analysis through Graph Neural Network Inductive Knowledge.\n",
        "\n",
        "At its core, MAGIK uses graph neural networks (GNNs) to learn patterns in particle movement and to infer trajectories across frames. This data-driven approach enables MAGIK to outperform traditional tracking methods in challenging conditions, such as dense particle fields, complex interaction dynamics, or non-Brownian motion.\n",
        "\n",
        "Thanks to its ability to learn and generalize motion priors, MAGIK is particularly effective in noisy or ambiguous experimental settings, making it a strong complement—or even an alternative—to classical tools like TrackPy and LapTrack.\n",
        "\n",
        "For more details, see the publication:  \n",
        "[Geometric Deep Learning Reveals the Spatiotemporal Features of Microscopic Motion](https://www.nature.com/articles/s42256-022-00595-0) – *Nat Mach Intell* **5**, 71–82 (2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training MAGIK with Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train MAGIK effectively, you first need to generate appropriate training data. Since the experimental dataset in this case features colloids undergoing Brownian motion, you'll use the `simulate_Brownian_trajs` function to produce groups of synthetic trajectories that replicate this behavior.\n",
        "\n",
        "**Note:** It is crucial that the simulated training data accurately reflect the motion characteristics of your experimental particles. If your system exhibits pure diffusion (Brownian motion), the training simulations should mirror that. Conversely, if your experimental data involve additional dynamics—such as drift, confinement, or driven flow (e.g. in nanofluidic systems)—these should be incorporated into the training data to ensure MAGIK learns the correct motion priors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "j1OEgXgKQOZl",
        "outputId": "d718a10c-227c-405e-cedd-1014c22c1cb0"
      },
      "outputs": [],
      "source": [
        "# Parameters for training dataset.\n",
        "train_dataset_size = 20 # Number of videos.\n",
        "train_number_particles = 30 # Number of particles per video.\n",
        "train_number_timesteps = 50 # Number of frames per video.\n",
        "train_fov_size = 64 # Size of the field of view.\n",
        "\n",
        "# Initiate a dataframe containing all the simulated trajectories.\n",
        "df_train_dataset = []\n",
        "for video_index in range(train_dataset_size):\n",
        "    # Simulate trajectories for one video.\n",
        "    sim_trajs_train_dataset = utils.simulate_Brownian_trajs(\n",
        "        num_particles=train_number_particles,\n",
        "        num_timesteps=train_number_timesteps,\n",
        "        fov_size=train_fov_size,\n",
        "        diffusion_std=0.5,\n",
        "    )\n",
        "    # Break trajectories going in/out of FOV.\n",
        "    sim_trajs_train_dataset_list = utils.traj_break(\n",
        "        trajs=sim_trajs_train_dataset,\n",
        "        fov_size=train_fov_size,\n",
        "        num_particles=train_number_particles,\n",
        "    )\n",
        "    # Make into dataframe with \"frame\" (which frame in the video),\n",
        "    #  label(which particle in that frame), set (which video).\n",
        "    for traj_index, traj in enumerate(sim_trajs_train_dataset_list):\n",
        "        df_traj = pd.DataFrame(\n",
        "            traj[:, 1:],\n",
        "            columns=[\"centroid-0\", \"centroid-1\"],\n",
        "        )\n",
        "        df_traj[\"frame\"] = traj[:, 0].astype(int)\n",
        "        df_traj[\"label\"] = traj_index\n",
        "        df_traj[\"set\"] = f\"{video_index}\"\n",
        "        df_train_dataset.append(df_traj)\n",
        "\n",
        "# Concatenate to dataframe.\n",
        "df_train_dataset = pd.concat(df_train_dataset, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize the trajectory coordinates between 0 and 1 by dividing for the fov size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize centroids between 0 and 1.\n",
        "norm_factor = np.array([train_fov_size, train_fov_size])\n",
        "df_train_dataset.loc[:, df_train_dataset.columns.str.contains(\"centroid\")] = (\n",
        "    df_train_dataset.loc[:, df_train_dataset.columns.str.contains(\"centroid\")] / norm_factor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HO6aEdSQOZm"
      },
      "source": [
        "To train MAGIK with the simulated trajectories, you need to produce a graph representation with the function `GraphFromTrajectories`, defined in the utility file `utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIg528NaQOZm",
        "outputId": "fbcbda7f-b87c-4dea-ee0e-8976ec4e604b"
      },
      "outputs": [],
      "source": [
        "# Instance the graph constructor.\n",
        "graph_constructor = utils.GraphFromTrajectories(\n",
        "    connectivity_radius=0.1,\n",
        "    max_frame_distance=3,\n",
        ")\n",
        "\n",
        "# Generate graph from training data using graph constructor.\n",
        "train_dataset_graph = graph_constructor(\n",
        "    df=df_train_dataset,\n",
        ")\n",
        "print(train_dataset_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTmRBM2CnNCm"
      },
      "source": [
        "Define the augmentation pipeline and the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQo0pfTbQOZm"
      },
      "outputs": [],
      "source": [
        "# Initialize the graph dataset class.\n",
        "# Specify augmentations with transform,\n",
        "# NodeDropout() should be last.\n",
        "train_dataset = utils.GraphDataset(\n",
        "    train_dataset_graph,\n",
        "    dataset_size=train_dataset_size,\n",
        "    Dt=5,\n",
        "    transform=Compose(\n",
        "        [\n",
        "            utils.RandomRotation(),\n",
        "            utils.RandomFlip(),\n",
        "            utils.NodeDropout(),\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Initialize the training data loader.\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1k6wjBQOZn"
      },
      "source": [
        "Define the hyperparameters of the architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBdnAwjgQOZn"
      },
      "outputs": [],
      "source": [
        "magik = dl.GraphToEdgeMAGIK(\n",
        "    [96,] * 4,1,\n",
        "    out_activation=torch.nn.Sigmoid\n",
        ")\n",
        "\n",
        "magik.encoder[0].configure(\n",
        "    hidden_features=[32, 64],\n",
        "    out_features=96,\n",
        "    out_activation=torch.nn.ReLU,\n",
        ")\n",
        "\n",
        "magik.encoder[1].configure(\n",
        "    hidden_features=[32, 64],\n",
        "    out_features=96,\n",
        "    out_activation=torch.nn.ReLU,\n",
        ")\n",
        "\n",
        "magik.head.configure(hidden_features=[64, 32]);\n",
        "\n",
        "classifier_magik = dl.BinaryClassifier(\n",
        "    model=magik,\n",
        "    optimizer=dl.Adam(lr=1e-3),\n",
        ").build()\n",
        "\n",
        "\n",
        "# Set training parameters and train.\n",
        "trainer_magik = dl.Trainer(max_epochs=200, accelerator=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd08re_GQOZn"
      },
      "source": [
        "Train the model or load preexisting weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define whether to force training even if weights exist.\n",
        "force_training = True\n",
        "\n",
        "# Define folder and model path.\n",
        "folder_name = \"MAGIK\"\n",
        "magik_model_path = os.path.join(folder_name, \"magik_weights.pth\")\n",
        "\n",
        "# Train or load weights.\n",
        "if not os.path.isfile(magik_model_path) or force_training:\n",
        "    print(\"Training MAGIK model (either forced or weights not found).\")\n",
        "\n",
        "    # Ensure save directory exists.\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "    # Train the model.\n",
        "    trainer_magik.fit(classifier_magik, train_loader)\n",
        "\n",
        "    # Save trained weights.\n",
        "    torch.save(magik.state_dict(), magik_model_path)\n",
        "    print(f\"Saved MAGIK weights to '{magik_model_path}'.\")\n",
        "else:\n",
        "    # Load pre-trained weights.\n",
        "    magik.load_state_dict(torch.load(magik_model_path, weights_only=True))\n",
        "    print(f\"Loaded preexisting MAGIK weights from '{magik_model_path}'.\")\n",
        "\n",
        "# Set the model to evaluation mode.\n",
        "classifier_magik.eval();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6ldS5tQOZn"
      },
      "source": [
        "### Linking Localizations in Simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll use the trajectories in the simulated video as the test dataset for the trained model of MAGIK. First, format the simulated dataframe. Then, convert the localizations corresponding to the simulated trajectories into a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename for compatibility with label format.\n",
        "df_sim_video_formatted = df_sim_video.rename(columns={\"x\": \"centroid-0\", \"y\": \"centroid-1\"})\n",
        "\n",
        "# Add label, set, and solution columns.\n",
        "df_sim_video_formatted[[\"label\", \"set\", \"solution\"]] = 0\n",
        "\n",
        "# Normalize coordinates to [0, 1].\n",
        "frame_height, frame_width, _ = sim_image.shape\n",
        "df_sim_video_formatted[[\"centroid-0\", \"centroid-1\"]] /= [frame_width, frame_height]\n",
        "\n",
        "# Generate a graph from graph_constructor. As test_graph returns a list of\n",
        "#  graphs, we select the first element from the list as it only has 1 element.\n",
        "sim_video_graph = graph_constructor(df=df_sim_video_formatted)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVWmgFQvQOZo"
      },
      "source": [
        "Apply the trained model of MAGIK to predict the edge features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "5lvw_Xc6QOZo",
        "outputId": "9828a7c9-6457-4e4d-9180-0970e845207e"
      },
      "outputs": [],
      "source": [
        "# Perform prediction on test graph.\n",
        "sim_trajs_edges_pred_method3  = classifier_magik(sim_video_graph)\n",
        "sim_trajs_edges_pred_method3  = sim_trajs_edges_pred_method3.detach().numpy() > 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the ground-truth edge features and, as a first performance metrics, use the F1-score for the classification of the edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the ground truth edges from the graph.\n",
        "sim_trajs_edges_gt = sim_video_graph.y\n",
        "\n",
        "# Compute the F1 score.\n",
        "F1 = f1_score(sim_trajs_edges_gt, sim_trajs_edges_pred_method3)\n",
        "print(f\"Test F1 score: {F1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MNRExDHQOZo"
      },
      "source": [
        "The edge feature can be used to obtain the predicted trajectories using the dedicate class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ht5ZdaFQOZp"
      },
      "outputs": [],
      "source": [
        "# Compute the trajectories from the predicted edges.\n",
        "trajectory_constructor = utils.ComputeTrajectories()\n",
        "sim_trajs_pred_method3 = trajectory_constructor(\n",
        "    sim_video_graph,\n",
        "    sim_trajs_edges_pred_method3.squeeze(),\n",
        ")\n",
        "\n",
        "# Convert the predicted trajectories to a list format.\n",
        "sim_trajs_pred_method3_list = utils.make_list(sim_trajs_pred_method3, sim_video_graph, sim_image_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a video with overlayed localizations and trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a video with the predicted trajectories.\n",
        "sim_video_method3_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = sim_trajs_pred_method3_list,\n",
        "    video = sim_video,\n",
        "    fov_size = sim_image_size,\n",
        "    trajs_gt_list = sim_trajs_gt_list,\n",
        ")\n",
        "\n",
        "sim_video_method3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Linking Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the overall performance of the tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate performance metrics.\n",
        "utils.trajectory_metrics(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method3_list,\n",
        "    eps=5,\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the reconstructed trajectories together with the groud truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Compute the total squared distance between all trajectories to match\n",
        "#  predicted trajectories with ground truth.\n",
        "matched_pairs, _, _ = utils.trajectory_assignment(\n",
        "    sim_trajs_gt_list,\n",
        "    sim_trajs_pred_method3_list,\n",
        "    eps=5,\n",
        ")\n",
        "\n",
        "# Plot the trajectories.\n",
        "utils.plot_trajectory_matches(sim_trajs_gt_list, sim_trajs_pred_method3_list, matched_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for all the trajectories and compare curves obtained for matching trajectories (same color)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = sim_trajs_pred_method3_list,\n",
        "    trajs_gt = sim_trajs_gt_list,\n",
        "    matched_pairs = matched_pairs,\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acu57jkGQOZq"
      },
      "source": [
        "### Linking Localizations in Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the same steps to track the experiment and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti6PUVMLQOZw"
      },
      "outputs": [],
      "source": [
        "# Rename for compatibility with label format.\n",
        "df_exp_video_formatted = df_exp_video.rename(columns={\"x\": \"centroid-0\", \"y\": \"centroid-1\"})\n",
        "\n",
        "# Add label, set, and solution columns.\n",
        "df_exp_video_formatted[[\"label\", \"set\", \"solution\"]] = 0\n",
        "\n",
        "# Normalize coordinates to [0, 1].\n",
        "frame_height, frame_width, _ = sim_image.shape\n",
        "df_exp_video_formatted[[\"centroid-0\", \"centroid-1\"]] /= [frame_width, frame_height]\n",
        "\n",
        "# Generate a graph from graph_constructor. As test_graph returns a list of\n",
        "#  graphs, we select the first element from the list as it only has 1 element.\n",
        "exp_video_graph = graph_constructor(df=df_exp_video_formatted)[0]\n",
        "\n",
        "# Perform prediction on graph.\n",
        "exp_trajs_edges_pred_method3  = classifier_magik(exp_video_graph)\n",
        "exp_trajs_edges_pred_method3  = exp_trajs_edges_pred_method3.detach().numpy() > 0.5\n",
        "\n",
        "# Compute the trajectories from the predicted edges.\n",
        "trajectory_constructor = utils.ComputeTrajectories()\n",
        "sim_trajs_pred_method3 = trajectory_constructor(\n",
        "    exp_video_graph,\n",
        "    exp_trajs_edges_pred_method3.squeeze(),\n",
        ")\n",
        "\n",
        "# Convert the predicted trajectories to a list format.\n",
        "exp_trajs_pred_method3_list = utils.make_list(sim_trajs_pred_method3, exp_video_graph, exp_image_size)\n",
        "\n",
        "# Create a video with the predicted trajectories.\n",
        "exp_video_method3_results = utils.make_video_with_trajs(\n",
        "    trajs_pred_list = exp_trajs_pred_method3_list,\n",
        "    video = exp_video,\n",
        "    fov_size = exp_image_size,\n",
        ")\n",
        "\n",
        "exp_video_method3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the time-averaged MSD for the trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_TAMSDs(\n",
        "    trajs_pred = exp_trajs_pred_method3_list,\n",
        ") "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplay_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4dd91e5328d494ab4ce1517fe9fec01": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a89634731d994290871e11f60cf0e5de",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 199/199 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">50/50</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:04 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">12.11it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 1 train_loss_step: 0.000133</span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">trainBinaryAccuracy_step: 1       </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 0.0175          </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">trainBinaryAccuracy_epoch: 0.993  </span>\n</pre>\n",
                  "text/plain": "Epoch 199/199 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m50/50\u001b[0m \u001b[38;5;245m0:00:04 • 0:00:00\u001b[0m \u001b[38;5;249m12.11it/s\u001b[0m \u001b[37mv_num: 1 train_loss_step: 0.000133\u001b[0m\n                                                                                 \u001b[37mtrainBinaryAccuracy_step: 1       \u001b[0m\n                                                                                 \u001b[37mtrain_loss_epoch: 0.0175          \u001b[0m\n                                                                                 \u001b[37mtrainBinaryAccuracy_epoch: 0.993  \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "a89634731d994290871e11f60cf0e5de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
